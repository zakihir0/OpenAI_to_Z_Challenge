{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ›ï¸ LIDARè€ƒå¤å­¦è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "\n",
    "## æ¦‚è¦\n",
    "ã“ã®Notebookã¯ã€LIDARãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸè€ƒå¤å­¦çš„æ§‹é€ ã®æ¤œå‡ºãƒ»è§£æã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "1. ğŸ›°ï¸ **ãƒ‡ãƒ¼ã‚¿å–å¾—** - .las/.lazç‚¹ç¾¤ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯åˆæˆãƒ‡ãƒ¼ã‚¿\n",
    "2. ğŸ§¹ **å‰å‡¦ç†** - DTM/DSMæŠ½å‡ºã€ãƒã‚¤ã‚ºé™¤å»\n",
    "3. ğŸ–¼ï¸ **ç”»åƒç”Ÿæˆ** - Hillshadeã€å‚¾æ–œã€ç­‰é«˜ç·š\n",
    "4. ğŸ”€ **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›** - ç”»åƒ+åœ°ç†æƒ…å ±\n",
    "5. ğŸ¤– **LLMè§£æ** - Deepseek Visionåˆ†æ\n",
    "6. ğŸ›ï¸ **è€ƒå¤å­¦çš„è§£é‡ˆ** - æ§‹é€ åˆ†é¡ã¨æ–‡åŒ–çš„è§£é‡ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install numpy matplotlib opencv-python scikit-image scipy pandas requests pillow\n",
    "!pip install rasterio geopandas shapely pyproj\n",
    "\n",
    "# å¯èƒ½ã§ã‚ã‚Œã°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "# !pip install laspy pdal open3d\n",
    "# !pip install openai  # OpenAI APIç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# ç§‘å­¦è¨ˆç®—\n",
    "from scipy import ndimage\n",
    "from skimage import filters, feature, measure\n",
    "\n",
    "# åœ°ç†ç©ºé–“å‡¦ç†\n",
    "try:\n",
    "    import rasterio\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point, Polygon\n",
    "    GIS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"GIS libraries not available - using basic functionality\")\n",
    "    GIS_AVAILABLE = False\n",
    "\n",
    "# LIDARå‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "try:\n",
    "    import laspy\n",
    "    LIDAR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"LIDAR libraries not available - using synthetic data\")\n",
    "    LIDAR_AVAILABLE = False\n",
    "\n",
    "# LLM APIï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "try:\n",
    "    import openai\n",
    "    LLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"OpenAI library not available - using mock analysis\")\n",
    "    LLM_AVAILABLE = False\n",
    "\n",
    "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›°ï¸ Step 1: ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_lidar_data(size=(512, 512)):\n",
    "    \"\"\"\n",
    "    åˆæˆLIDARãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ\n",
    "    å®Ÿéš›ã®LIDARãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã®ãƒ†ã‚¹ãƒˆç”¨\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ åˆæˆLIDARãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...\")\n",
    "    \n",
    "    # åŸºæœ¬åœ°å½¢ã®ç”Ÿæˆ\n",
    "    x, y = np.meshgrid(np.linspace(0, 100, size[1]), np.linspace(0, 100, size[0]))\n",
    "    \n",
    "    # è¤‡é›‘ãªåœ°å½¢ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "    elevation = (\n",
    "        100 +  # åŸºæº–æ¨™é«˜\n",
    "        30 * np.sin(x / 10) * np.cos(y / 15) +  # å¤§ããªåœ°å½¢èµ·ä¼\n",
    "        15 * np.sin(x / 5) * np.sin(y / 8) +    # ä¸­è¦æ¨¡èµ·ä¼\n",
    "        5 * np.random.randn(*size)              # ãƒã‚¤ã‚º\n",
    "    )\n",
    "    \n",
    "    # è€ƒå¤å­¦çš„æ§‹é€ ã®è¿½åŠ \n",
    "    print(\"ğŸ›ï¸ è€ƒå¤å­¦çš„æ§‹é€ ã‚’è¿½åŠ ä¸­...\")\n",
    "    \n",
    "    # 1. å††å½¢åœŸå·¥ï¼ˆCasarabeæ§˜å¼ï¼‰\n",
    "    center_y, center_x = size[0] // 3, size[1] // 3\n",
    "    radius = 25\n",
    "    y_coords, x_coords = np.ogrid[:size[0], :size[1]]\n",
    "    circle_mask = (x_coords - center_x)**2 + (y_coords - center_y)**2 <= radius**2\n",
    "    elevation[circle_mask] += 2.5  # ç››ã‚ŠåœŸ\n",
    "    \n",
    "    # 2. ç·šå½¢æ§‹é€ ï¼ˆå¤ä»£é“è·¯ï¼‰\n",
    "    elevation[size[0]//2:size[0]//2+3, :] += 1.0\n",
    "    \n",
    "    # 3. ãƒã‚¦ãƒ³ãƒ‰ï¼ˆå„€å¼çš„æ§‹é€ ï¼‰\n",
    "    mound_y, mound_x = size[0] * 2 // 3, size[1] * 2 // 3\n",
    "    mound_radius = 15\n",
    "    mound_mask = (x_coords - mound_x)**2 + (y_coords - mound_y)**2 <= mound_radius**2\n",
    "    mound_height = 4.0 * np.exp(-((x_coords - mound_x)**2 + (y_coords - mound_y)**2) / (mound_radius**2 / 3))\n",
    "    elevation[mound_mask] += mound_height[mound_mask]\n",
    "    \n",
    "    # 4. çŸ©å½¢æ§‹é€ ï¼ˆå±…ä½åŒºç”»ï¼‰\n",
    "    rect_y1, rect_y2 = size[0]//4, size[0]//4 + 20\n",
    "    rect_x1, rect_x2 = size[1]//2, size[1]//2 + 30\n",
    "    elevation[rect_y1:rect_y2, rect_x1:rect_x2] += 1.5\n",
    "    \n",
    "    print(f\"âœ… åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {size[0]}x{size[1]}\")\n",
    "    return elevation\n",
    "\n",
    "# ã‚µã‚¤ãƒˆæƒ…å ±ã®è¨­å®š\n",
    "site_info = {\n",
    "    'name': 'Amazon Test Archaeological Site',\n",
    "    'region': 'Western Amazon Basin',\n",
    "    'coordinates': [-8.5, -63.2],  # ç·¯åº¦ã€çµŒåº¦\n",
    "    'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'expected_features': ['earthworks', 'mounds', 'linear_features']\n",
    "}\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¾ãŸã¯èª­ã¿è¾¼ã¿\n",
    "if LIDAR_AVAILABLE:\n",
    "    # å®Ÿéš›ã®LIDARãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ\n",
    "    # lidar_file = \"path/to/your/file.las\"\n",
    "    # las_data = laspy.read(lidar_file)\n",
    "    # elevation = process_lidar_to_dtm(las_data)\n",
    "    elevation = create_synthetic_lidar_data()\n",
    "else:\n",
    "    elevation = create_synthetic_lidar_data()\n",
    "\n",
    "print(f\"ğŸ“Š æ¨™é«˜ãƒ‡ãƒ¼ã‚¿: æœ€å°={elevation.min():.1f}m, æœ€å¤§={elevation.max():.1f}m, å¹³å‡={elevation.mean():.1f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Step 2: å‰å‡¦ç†ã¨DTM/DSMæŠ½å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_elevation_data(elevation):\n",
    "    \"\"\"\n",
    "    æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§¹ å‰å‡¦ç†ã‚’å®Ÿè¡Œä¸­...\")\n",
    "    \n",
    "    # 1. ãƒã‚¤ã‚ºé™¤å»ï¼ˆã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ï¼‰\n",
    "    elevation_smooth = ndimage.gaussian_filter(elevation, sigma=1.0)\n",
    "    \n",
    "    # 2. DTMï¼ˆåœ°è¡¨é¢ï¼‰ã¨DSMï¼ˆè¡¨é¢ï¼‰ã®åˆ†é›¢\n",
    "    # å®Ÿéš›ã®å‡¦ç†ã§ã¯æ¤ç”Ÿé™¤å»ãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯ç°¡ç•¥åŒ–\n",
    "    dtm = elevation_smooth  # åœ°è¡¨é¢\n",
    "    dsm = elevation + np.random.rand(*elevation.shape) * 2  # è¡¨é¢ï¼ˆæ¤ç”Ÿå«ã‚€ï¼‰\n",
    "    \n",
    "    # 3. åœ°å½¢æ´¾ç”Ÿãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®—\n",
    "    dy, dx = np.gradient(dtm)\n",
    "    slope = np.degrees(np.arctan(np.sqrt(dx**2 + dy**2)))\n",
    "    aspect = np.degrees(np.arctan2(-dx, dy)) % 360\n",
    "    \n",
    "    # 4. æ›²ç‡ã®è¨ˆç®—\n",
    "    dyy, dyx = np.gradient(dy)\n",
    "    dxy, dxx = np.gradient(dx)\n",
    "    curvature = (dxx + dyy) / (1 + dx**2 + dy**2)**(3/2)\n",
    "    \n",
    "    # 5. å±€æ‰€èµ·ä¼\n",
    "    local_relief = dtm - ndimage.uniform_filter(dtm, size=20)\n",
    "    \n",
    "    terrain_data = {\n",
    "        'dtm': dtm,\n",
    "        'dsm': dsm,\n",
    "        'slope': slope,\n",
    "        'aspect': aspect,\n",
    "        'curvature': curvature,\n",
    "        'local_relief': local_relief,\n",
    "        'gradient_x': dx,\n",
    "        'gradient_y': dy\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… å‰å‡¦ç†å®Œäº†\")\n",
    "    return terrain_data\n",
    "\n",
    "# å‰å‡¦ç†ã®å®Ÿè¡Œ\n",
    "terrain_data = preprocess_elevation_data(elevation)\n",
    "\n",
    "print(f\"ğŸ“ˆ å‚¾æ–œ: æœ€å°={terrain_data['slope'].min():.1f}Â°, æœ€å¤§={terrain_data['slope'].max():.1f}Â°\")\n",
    "print(f\"ğŸ§­ æ–¹ä½: æœ€å°={terrain_data['aspect'].min():.1f}Â°, æœ€å¤§={terrain_data['aspect'].max():.1f}Â°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ Step 3: é™°å½±èµ·ä¼å›³ã¨å¯è¦–åŒ–ç”»åƒã®ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hillshade(dtm, azimuth=315, altitude=45):\n",
    "    \"\"\"\n",
    "    é™°å½±èµ·ä¼å›³ï¼ˆHillshadeï¼‰ã®ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    dy, dx = np.gradient(dtm)\n",
    "    slope = np.arctan(np.sqrt(dx**2 + dy**2))\n",
    "    aspect = np.arctan2(-dx, dy)\n",
    "    \n",
    "    azimuth_rad = np.radians(azimuth)\n",
    "    altitude_rad = np.radians(altitude)\n",
    "    \n",
    "    hillshade = np.sin(altitude_rad) * np.sin(slope) + \\\n",
    "               np.cos(altitude_rad) * np.cos(slope) * \\\n",
    "               np.cos(azimuth_rad - aspect)\n",
    "    \n",
    "    return ((hillshade + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "def create_analysis_images(terrain_data, site_info):\n",
    "    \"\"\"\n",
    "    è§£æç”¨ç”»åƒã®ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ–¼ï¸ è§£æç”»åƒã‚’ç”Ÿæˆä¸­...\")\n",
    "    \n",
    "    # é™°å½±èµ·ä¼å›³\n",
    "    hillshade = generate_hillshade(terrain_data['dtm'])\n",
    "    \n",
    "    # 4ã¤ã®ä¸»è¦ç”»åƒã‚’ç”Ÿæˆ\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    \n",
    "    # 1. é™°å½±èµ·ä¼å›³\n",
    "    ax1.imshow(hillshade, cmap='gray', extent=[0, 100, 0, 100])\n",
    "    ax1.set_title('Hillshade Analysis\\né™°å½±èµ·ä¼å›³', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Distance (km)')\n",
    "    ax1.set_ylabel('Distance (km)')\n",
    "    \n",
    "    # 2. æ¨™é«˜ï¼‹ç­‰é«˜ç·š\n",
    "    im2 = ax2.imshow(terrain_data['dtm'], cmap='terrain', extent=[0, 100, 0, 100])\n",
    "    contours = ax2.contour(terrain_data['dtm'], levels=15, colors='black', alpha=0.6, linewidths=0.8)\n",
    "    ax2.clabel(contours, inline=True, fontsize=8, fmt='%d m')\n",
    "    ax2.set_title('Elevation + Contours\\næ¨™é«˜ï¼‹ç­‰é«˜ç·š', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Distance (km)')\n",
    "    ax2.set_ylabel('Distance (km)')\n",
    "    plt.colorbar(im2, ax=ax2, shrink=0.8, label='Elevation (m)')\n",
    "    \n",
    "    # 3. å‚¾æ–œè§£æ\n",
    "    im3 = ax3.imshow(terrain_data['slope'], cmap='plasma', extent=[0, 100, 0, 100])\n",
    "    ax3.set_title('Slope Analysis\\nå‚¾æ–œè§£æ', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Distance (km)')\n",
    "    ax3.set_ylabel('Distance (km)')\n",
    "    plt.colorbar(im3, ax=ax3, shrink=0.8, label='Slope (Â°)')\n",
    "    \n",
    "    # 4. å±€æ‰€èµ·ä¼\n",
    "    im4 = ax4.imshow(terrain_data['local_relief'], cmap='RdBu_r', extent=[0, 100, 0, 100])\n",
    "    ax4.set_title('Local Relief\\nå±€æ‰€èµ·ä¼', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Distance (km)')\n",
    "    ax4.set_ylabel('Distance (km)')\n",
    "    plt.colorbar(im4, ax=ax4, shrink=0.8, label='Relief (m)')\n",
    "    \n",
    "    plt.suptitle(f'LIDAR Archaeological Analysis: {site_info[\"name\"]}\\n'\n",
    "                f'Coordinates: {site_info[\"coordinates\"]}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ç”»åƒã‚’ä¿å­˜\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    image_path = f'lidar_analysis_{timestamp}.png'\n",
    "    plt.savefig(image_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… è§£æç”»åƒä¿å­˜: {image_path}\")\n",
    "    return {\n",
    "        'hillshade': hillshade,\n",
    "        'analysis_image': image_path\n",
    "    }\n",
    "\n",
    "# ç”»åƒç”Ÿæˆ\n",
    "images = create_analysis_images(terrain_data, site_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 4: è€ƒå¤å­¦çš„æ§‹é€ ã®æ¤œå‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_archaeological_structures(terrain_data):\n",
    "    \"\"\"\n",
    "    è€ƒå¤å­¦çš„æ§‹é€ ã®è‡ªå‹•æ¤œå‡º\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” è€ƒå¤å­¦çš„æ§‹é€ ã‚’æ¤œå‡ºä¸­...\")\n",
    "    \n",
    "    dtm = terrain_data['dtm']\n",
    "    local_relief = terrain_data['local_relief']\n",
    "    hillshade = generate_hillshade(dtm)\n",
    "    \n",
    "    structures = {\n",
    "        'circular_features': [],\n",
    "        'linear_features': [],\n",
    "        'mounds': [],\n",
    "        'depressions': []\n",
    "    }\n",
    "    \n",
    "    # 1. å††å½¢æ§‹é€ ã®æ¤œå‡ºï¼ˆHoughå††å¤‰æ›ï¼‰\n",
    "    edges = cv2.Canny(hillshade, 50, 150)\n",
    "    circles = cv2.HoughCircles(\n",
    "        edges, cv2.HOUGH_GRADIENT, dp=1, minDist=30,\n",
    "        param1=50, param2=30, minRadius=5, maxRadius=50\n",
    "    )\n",
    "    \n",
    "    if circles is not None:\n",
    "        circles = np.round(circles[0, :]).astype(\"int\")\n",
    "        for (x, y, r) in circles:\n",
    "            structures['circular_features'].append({\n",
    "                'center': (y, x),\n",
    "                'radius': int(r),\n",
    "                'confidence': 0.8,\n",
    "                'type': 'earthwork'\n",
    "            })\n",
    "    \n",
    "    # 2. ç·šå½¢æ§‹é€ ã®æ¤œå‡ºï¼ˆHoughç›´ç·šå¤‰æ›ï¼‰\n",
    "    lines = cv2.HoughLinesP(\n",
    "        edges, rho=1, theta=np.pi/180, threshold=80,\n",
    "        minLineLength=40, maxLineGap=8\n",
    "    )\n",
    "    \n",
    "    if lines is not None:\n",
    "        for line in lines[:10]:  # ä¸Šä½10æœ¬\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            length = np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "            angle = np.degrees(np.arctan2(y2-y1, x2-x1))\n",
    "            \n",
    "            structures['linear_features'].append({\n",
    "                'start': (y1, x1),\n",
    "                'end': (y2, x2),\n",
    "                'length': float(length),\n",
    "                'angle': float(angle),\n",
    "                'confidence': 0.7,\n",
    "                'type': 'road_or_canal'\n",
    "            })\n",
    "    \n",
    "    # 3. ãƒã‚¦ãƒ³ãƒ‰æ¤œå‡ºï¼ˆå±€æ‰€æœ€å¤§å€¤ï¼‰\n",
    "    local_maxima = ndimage.maximum_filter(local_relief, size=15) == local_relief\n",
    "    local_maxima_coords = np.where(local_maxima)\n",
    "    \n",
    "    threshold = np.std(local_relief) * 1.5\n",
    "    for y, x in zip(*local_maxima_coords):\n",
    "        height = local_relief[y, x]\n",
    "        if height > threshold:\n",
    "            structures['mounds'].append({\n",
    "                'center': (y, x),\n",
    "                'height': float(height),\n",
    "                'confidence': min(height / threshold, 1.0),\n",
    "                'type': 'ceremonial_mound'\n",
    "            })\n",
    "    \n",
    "    # 4. çªªåœ°æ¤œå‡ºï¼ˆå±€æ‰€æœ€å°å€¤ï¼‰\n",
    "    local_minima = ndimage.minimum_filter(local_relief, size=15) == local_relief\n",
    "    local_minima_coords = np.where(local_minima)\n",
    "    \n",
    "    threshold_neg = -np.std(local_relief) * 1.5\n",
    "    for y, x in zip(*local_minima_coords):\n",
    "        depth = local_relief[y, x]\n",
    "        if depth < threshold_neg:\n",
    "            structures['depressions'].append({\n",
    "                'center': (y, x),\n",
    "                'depth': float(abs(depth)),\n",
    "                'confidence': min(abs(depth) / abs(threshold_neg), 1.0),\n",
    "                'type': 'quarry_or_pond'\n",
    "            })\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±\n",
    "    total_structures = sum(len(structures[key]) for key in structures)\n",
    "    structures['summary'] = {\n",
    "        'total_structures': total_structures,\n",
    "        'circular_count': len(structures['circular_features']),\n",
    "        'linear_count': len(structures['linear_features']),\n",
    "        'mound_count': len(structures['mounds']),\n",
    "        'depression_count': len(structures['depressions'])\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… æ§‹é€ æ¤œå‡ºå®Œäº†: ç·è¨ˆ{total_structures}å€‹\")\n",
    "    print(f\"   - å††å½¢æ§‹é€ : {len(structures['circular_features'])}å€‹\")\n",
    "    print(f\"   - ç·šå½¢æ§‹é€ : {len(structures['linear_features'])}å€‹\")\n",
    "    print(f\"   - ãƒã‚¦ãƒ³ãƒ‰: {len(structures['mounds'])}å€‹\")\n",
    "    print(f\"   - çªªåœ°: {len(structures['depressions'])}å€‹\")\n",
    "    \n",
    "    return structures\n",
    "\n",
    "# æ§‹é€ æ¤œå‡ºã®å®Ÿè¡Œ\n",
    "detected_structures = detect_archaeological_structures(terrain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ ã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detected_structures(terrain_data, structures, site_info):\n",
    "    \"\"\"\n",
    "    æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ ã®å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    # é™°å½±èµ·ä¼å›³ã‚’ãƒ™ãƒ¼ã‚¹ã«\n",
    "    hillshade = generate_hillshade(terrain_data['dtm'])\n",
    "    ax.imshow(hillshade, cmap='gray', alpha=0.8, extent=[0, 100, 0, 100])\n",
    "    \n",
    "    # è‰²ã®å®šç¾©\n",
    "    colors = {\n",
    "        'circular_features': 'red',\n",
    "        'linear_features': 'blue',\n",
    "        'mounds': 'orange',\n",
    "        'depressions': 'purple'\n",
    "    }\n",
    "    \n",
    "    labels = {\n",
    "        'circular_features': 'Circular Earthworks\\nå††å½¢åœŸå·¥',\n",
    "        'linear_features': 'Linear Features\\nç·šå½¢æ§‹é€ ',\n",
    "        'mounds': 'Mounds\\nãƒã‚¦ãƒ³ãƒ‰',\n",
    "        'depressions': 'Depressions\\nçªªåœ°'\n",
    "    }\n",
    "    \n",
    "    # æ§‹é€ ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    for structure_type, color in colors.items():\n",
    "        features = structures[structure_type]\n",
    "        if not features:\n",
    "            continue\n",
    "            \n",
    "        if structure_type == 'circular_features':\n",
    "            for feature in features:\n",
    "                center = feature['center']\n",
    "                radius = feature['radius']\n",
    "                # ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã‚’åœ°ç†åº§æ¨™ã«å¤‰æ›\n",
    "                x = (center[1] / hillshade.shape[1]) * 100\n",
    "                y = 100 - (center[0] / hillshade.shape[0]) * 100\n",
    "                circle = plt.Circle((x, y), radius * 100 / hillshade.shape[1], \n",
    "                                  fill=False, color=color, linewidth=2, alpha=0.8)\n",
    "                ax.add_patch(circle)\n",
    "                \n",
    "        elif structure_type == 'linear_features':\n",
    "            for feature in features:\n",
    "                start = feature['start']\n",
    "                end = feature['end']\n",
    "                x1 = (start[1] / hillshade.shape[1]) * 100\n",
    "                y1 = 100 - (start[0] / hillshade.shape[0]) * 100\n",
    "                x2 = (end[1] / hillshade.shape[1]) * 100\n",
    "                y2 = 100 - (end[0] / hillshade.shape[0]) * 100\n",
    "                ax.plot([x1, x2], [y1, y2], color=color, linewidth=3, alpha=0.8)\n",
    "                \n",
    "        else:  # mounds, depressions\n",
    "            x_coords, y_coords = [], []\n",
    "            for feature in features:\n",
    "                center = feature['center']\n",
    "                x = (center[1] / hillshade.shape[1]) * 100\n",
    "                y = 100 - (center[0] / hillshade.shape[0]) * 100\n",
    "                x_coords.append(x)\n",
    "                y_coords.append(y)\n",
    "            \n",
    "            if x_coords:\n",
    "                ax.scatter(x_coords, y_coords, c=color, s=80, alpha=0.8, \n",
    "                          edgecolors='black', linewidth=1, label=labels[structure_type])\n",
    "    \n",
    "    # ãƒ¬ã‚¸ã‚§ãƒ³ãƒ‰ã¨ã‚¿ã‚¤ãƒˆãƒ«\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1, 1))\n",
    "    ax.set_title(f'Archaeological Structure Detection\\nè€ƒå¤å­¦çš„æ§‹é€ æ¤œå‡º\\n{site_info[\"name\"]}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Distance (km)')\n",
    "    ax.set_ylabel('Distance (km)')\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã§è¡¨ç¤º\n",
    "    summary = structures['summary']\n",
    "    stats_text = f\"\"\"æ¤œå‡ºçµ±è¨ˆ:\n",
    "ç·æ§‹é€ æ•°: {summary['total_structures']}\n",
    "å††å½¢: {summary['circular_count']}\n",
    "ç·šå½¢: {summary['linear_count']}\n",
    "ãƒã‚¦ãƒ³ãƒ‰: {summary['mound_count']}\n",
    "çªªåœ°: {summary['depression_count']}\"\"\"\n",
    "    \n",
    "    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10,\n",
    "           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ç”»åƒä¿å­˜\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    structure_image = f'structure_detection_{timestamp}.png'\n",
    "    plt.savefig(structure_image, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… æ§‹é€ æ¤œå‡ºç”»åƒä¿å­˜: {structure_image}\")\n",
    "    return structure_image\n",
    "\n",
    "# æ§‹é€ å¯è¦–åŒ–\n",
    "structure_image = visualize_detected_structures(terrain_data, detected_structures, site_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ Step 5: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_multimodal_input(images, site_info, terrain_data, structures):\n",
    "    \"\"\"\n",
    "    LLMè§£æç”¨ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã‚’æº–å‚™\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”€ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã‚’æ§‹ç¯‰ä¸­...\")\n",
    "    \n",
    "    # åœ°ç†çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "    geographic_context = {\n",
    "        'site_name': site_info['name'],\n",
    "        'region': site_info['region'],\n",
    "        'coordinates': site_info['coordinates'],\n",
    "        'latitude': site_info['coordinates'][0],\n",
    "        'longitude': site_info['coordinates'][1]\n",
    "    }\n",
    "    \n",
    "    # åœ°å½¢çµ±è¨ˆ\n",
    "    terrain_stats = {\n",
    "        'elevation_range': {\n",
    "            'min': float(terrain_data['dtm'].min()),\n",
    "            'max': float(terrain_data['dtm'].max()),\n",
    "            'mean': float(terrain_data['dtm'].mean()),\n",
    "            'std': float(terrain_data['dtm'].std())\n",
    "        },\n",
    "        'slope_stats': {\n",
    "            'mean': float(terrain_data['slope'].mean()),\n",
    "            'max': float(terrain_data['slope'].max())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # æ§‹é€ çµ±è¨ˆ\n",
    "    structure_summary = structures['summary']\n",
    "    \n",
    "    # åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "    analysis_prompt = f\"\"\"\n",
    "LIDARè€ƒå¤å­¦è§£æ: {site_info['name']}\n",
    "\n",
    "ã‚µã‚¤ãƒˆæƒ…å ±:\n",
    "- ä½ç½®: {site_info['coordinates'][0]:.3f}, {site_info['coordinates'][1]:.3f}\n",
    "- åœ°åŸŸ: {site_info['region']}\n",
    "- è§£ææ—¥: {site_info['date']}\n",
    "\n",
    "åœ°å½¢ç‰¹æ€§:\n",
    "- æ¨™é«˜ç¯„å›²: {terrain_stats['elevation_range']['min']:.1f} - {terrain_stats['elevation_range']['max']:.1f}m\n",
    "- å¹³å‡æ¨™é«˜: {terrain_stats['elevation_range']['mean']:.1f}m\n",
    "- å¹³å‡å‚¾æ–œ: {terrain_stats['slope_stats']['mean']:.1f}Â°\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ :\n",
    "- ç·æ§‹é€ æ•°: {structure_summary['total_structures']}\n",
    "- å††å½¢æ§‹é€ : {structure_summary['circular_count']} (åœŸå·¥ã€å„€å¼çš„å›²ã„è¾¼ã¿)\n",
    "- ç·šå½¢æ§‹é€ : {structure_summary['linear_count']} (é“è·¯ã€é‹æ²³ã€åœŸæ‰‹é“)\n",
    "- ãƒã‚¦ãƒ³ãƒ‰: {structure_summary['mound_count']} (å„€å¼çš„ã€åŸ‹è‘¬ç”¨æ§‹é€ )\n",
    "- çªªåœ°: {structure_summary['depression_count']} (æ¡çŸ³å ´ã€è²¯æ°´æ± )\n",
    "\n",
    "è€ƒå¤å­¦çš„è©•ä¾¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼š\n",
    "1. æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ–‡åŒ–çš„æ„ç¾©\n",
    "2. æ—¢çŸ¥ã®ã‚¢ãƒã‚¾ãƒ³è€ƒå¤å­¦æ–‡åŒ–ã¨ã®æ¯”è¼ƒ\n",
    "3. æ¨å®šã•ã‚Œã‚‹æ™‚ä»£ã¨æ©Ÿèƒ½\n",
    "4. ç¾åœ°èª¿æŸ»ã®æ¨å¥¨äº‹é …\n",
    "\"\"\"\n",
    "    \n",
    "    multimodal_input = {\n",
    "        'geographic_context': geographic_context,\n",
    "        'terrain_statistics': terrain_stats,\n",
    "        'structure_summary': structure_summary,\n",
    "        'analysis_prompt': analysis_prompt,\n",
    "        'images': images\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›æ§‹ç¯‰å®Œäº†\")\n",
    "    return multimodal_input\n",
    "\n",
    "# ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã®æº–å‚™\n",
    "multimodal_input = prepare_multimodal_input(images, site_info, terrain_data, detected_structures)\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¡¨ç¤º\n",
    "print(\"ğŸ“ LLMè§£æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\")\n",
    "print(multimodal_input['analysis_prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 6: LLM/ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«è§£æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_analysis(multimodal_input):\n",
    "    \"\"\"\n",
    "    LLM/ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹è§£æ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¤– LLMè§£æã‚’å®Ÿè¡Œä¸­...\")\n",
    "    \n",
    "    if LLM_AVAILABLE:\n",
    "        # å®Ÿéš›ã®LLM APIå‘¼ã³å‡ºã—\n",
    "        try:\n",
    "            client = openai.OpenAI(\n",
    "                api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                base_url=os.getenv('OPENAI_BASE_URL', 'https://openrouter.ai/api/v1')\n",
    "            )\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=os.getenv('OPENAI_MODEL', 'deepseek/deepseek-r1-0528:free'),\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"ã‚ãªãŸã¯ã‚¢ãƒã‚¾ãƒ³è€ƒå¤å­¦ã®å°‚é–€å®¶ã§ã™ã€‚LIDARè§£æçµæœã‚’åŸºã«è€ƒå¤å­¦çš„è§£é‡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": multimodal_input['analysis_prompt']\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=1500\n",
    "            )\n",
    "            \n",
    "            llm_analysis = response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LLM APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            llm_analysis = generate_mock_analysis(multimodal_input)\n",
    "    else:\n",
    "        # ãƒ¢ãƒƒã‚¯LLMè§£æ\n",
    "        llm_analysis = generate_mock_analysis(multimodal_input)\n",
    "    \n",
    "    print(\"âœ… LLMè§£æå®Œäº†\")\n",
    "    return llm_analysis\n",
    "\n",
    "def generate_mock_analysis(multimodal_input):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒƒã‚¯å°‚é–€å®¶è§£æã®ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    summary = multimodal_input['structure_summary']\n",
    "    geo = multimodal_input['geographic_context']\n",
    "    \n",
    "    mock_analysis = f\"\"\"\n",
    "ğŸ›ï¸ è€ƒå¤å­¦çš„å°‚é–€å®¶è§£æ: {geo['site_name']}\n",
    "\n",
    "ğŸ“ ã‚µã‚¤ãƒˆè©•ä¾¡:\n",
    "ä½ç½® {geo['coordinates'][0]:.3f}, {geo['coordinates'][1]:.3f} ã«ãŠã‘ã‚‹ LIDAR è§£æã«ã‚ˆã‚Šã€\n",
    "{summary['total_structures']} ã®äººç‚ºçš„æ§‹é€ ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚\n",
    "\n",
    "ğŸ” æ§‹é€ åˆ†æ:\n",
    "â€¢ å††å½¢æ§‹é€  ({summary['circular_count']}å€‹): Casarabeæ–‡åŒ–æ§˜å¼ã®åœŸå·¥ã«é¡ä¼¼\n",
    "â€¢ ç·šå½¢æ§‹é€  ({summary['linear_count']}å€‹): å¤ä»£é“è·¯ç¶²ã¾ãŸã¯é‹æ²³ã‚·ã‚¹ãƒ†ãƒ \n",
    "â€¢ ãƒã‚¦ãƒ³ãƒ‰ ({summary['mound_count']}å€‹): å„€å¼çš„ãƒ»åŸ‹è‘¬ç›®çš„ã®æ§‹é€ \n",
    "â€¢ çªªåœ° ({summary['depression_count']}å€‹): è³‡æºæ¡å–ã¾ãŸã¯è²¯æ°´æ©Ÿèƒ½\n",
    "\n",
    "ğŸº æ–‡åŒ–çš„è§£é‡ˆ:\n",
    "æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä»¥ä¸‹ã®æ—¢çŸ¥æ–‡åŒ–ã¨ã®é¡ä¼¼æ€§ã‚’ç¤ºã—ã¾ã™ï¼š\n",
    "- Casarabeæ–‡åŒ– (500-1400 CE): å¹¾ä½•å­¦çš„åœŸå·¥è¤‡åˆä½“\n",
    "- Marajoaraæ–‡åŒ– (400-1200 CE): å„€å¼çš„ã‚»ãƒ³ã‚¿ãƒ¼\n",
    "- Llanos de Mojos: è€•ä½œã‚·ã‚¹ãƒ†ãƒ ã¨æ£®æ—å³¶\n",
    "\n",
    "â° æ¨å®šæ™‚ä»£:\n",
    "æ§‹é€ ã®ç‰¹å¾´ã‹ã‚‰ã€å¾ŒæœŸåœŸå™¨æ™‚ä»£ (1000-1500 CE) ã®è¤‡åˆç¤¾ä¼šã«ã‚ˆã‚‹\n",
    "å¤§è¦æ¨¡ãªæ™¯è¦³æ”¹å¤‰ã‚’ç¤ºå”†ã€‚\n",
    "\n",
    "ğŸ¯ æ©Ÿèƒ½æ¨å®š:\n",
    "- å††å½¢æ§‹é€ : å„€å¼çš„ãƒ»é˜²å¾¡çš„å›²ã„è¾¼ã¿\n",
    "- ç·šå½¢æ§‹é€ : äº¤é€šãƒ»æµé€šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "- ãƒã‚¦ãƒ³ãƒ‰: ç¤¾ä¼šéšå±¤ãƒ»å®—æ•™çš„æ©Ÿèƒ½\n",
    "- è¤‡åˆã‚·ã‚¹ãƒ†ãƒ : å¤šæ©Ÿèƒ½çš„æ™¯è¦³åˆ©ç”¨\n",
    "\n",
    "ğŸ“‹ ç¾åœ°èª¿æŸ»æ¨å¥¨:\n",
    "1. ğŸ”¬ åœ°ä¸­ãƒ¬ãƒ¼ãƒ€ãƒ¼èª¿æŸ»ã«ã‚ˆã‚‹åœ°ä¸‹æ§‹é€ ç¢ºèª\n",
    "2. ğŸ§ª æ”¾å°„æ€§ç‚­ç´ å¹´ä»£æ¸¬å®šç”¨ã‚µãƒ³ãƒ—ãƒ«æ¡å–\n",
    "3. ğŸº è¡¨é¢ã‚»ãƒ©ãƒŸãƒƒã‚¯åˆ†æ\n",
    "4. ğŸŒ± å¤ç’°å¢ƒå¾©å…ƒã®ãŸã‚ã®ç’°å¢ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "5. ğŸ‘¥ åœ°åŸŸå…ˆä½æ°‘ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨ã®å”è­°\n",
    "6. ğŸ“¸ ãƒ•ã‚©ãƒˆã‚°ãƒ©ãƒ¡ãƒˆãƒªãƒ¼ãƒ»3Dã‚¹ã‚­ãƒ£ãƒ³è¨˜éŒ²\n",
    "\n",
    "âš ï¸ ä¿è­·æªç½®:\n",
    "æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ ã®é«˜ã„å¯†åº¦ã¨å¤šæ§˜æ€§ã‹ã‚‰ã€å³åº§ã®ä¿è­·æªç½®ãŒå¿…è¦ã€‚\n",
    "åœ°åŸŸè€ƒå¤å­¦å½“å±€ã¨ã®é€£æºã‚’å¼·ãæ¨å¥¨ã€‚\n",
    "\n",
    "ğŸŒŸ è€ƒå¤å­¦çš„æ„ç¾©:\n",
    "ã“ã®ã‚µã‚¤ãƒˆã¯ã€ã‚¢ãƒã‚¾ãƒ³ç›†åœ°ã®è¤‡åˆç¤¾ä¼šã«ã‚ˆã‚‹æ™¯è¦³æ”¹å¤‰ã®ç†è§£ã«\n",
    "é‡è¦ãªè²¢çŒ®ã‚’ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€å›½éš›çš„ãªç ”ç©¶ä¾¡å€¤ã‚’æœ‰ã—ã¾ã™ã€‚\n",
    "\"\"\"\n",
    "    \n",
    "    return mock_analysis\n",
    "\n",
    "# LLMè§£æã®å®Ÿè¡Œ\n",
    "archaeological_interpretation = run_llm_analysis(multimodal_input)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¤– LLMè€ƒå¤å­¦è§£æçµæœ:\")\n",
    "print(\"=\"*60)\n",
    "print(archaeological_interpretation)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ Step 7: è€ƒå¤å­¦çš„è©•ä¾¡ã¨æ¨å¥¨äº‹é …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_archaeological_score(structures, terrain_data):\n",
    "    \"\"\"\n",
    "    è€ƒå¤å­¦çš„é‡è¦åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—\n",
    "    \"\"\"\n",
    "    summary = structures['summary']\n",
    "    \n",
    "    # æ§‹é€ æ•°ã‚¹ã‚³ã‚¢ (0-0.4)\n",
    "    structure_score = min(summary['total_structures'] / 50.0, 0.4)\n",
    "    \n",
    "    # å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ (0-0.3)\n",
    "    structure_types = sum([\n",
    "        1 for count in [\n",
    "            summary['circular_count'],\n",
    "            summary['linear_count'],\n",
    "            summary['mound_count'],\n",
    "            summary['depression_count']\n",
    "        ] if count > 0\n",
    "    ])\n",
    "    diversity_score = (structure_types / 4.0) * 0.3\n",
    "    \n",
    "    # åœ°å½¢è¤‡é›‘æ€§ã‚¹ã‚³ã‚¢ (0-0.2)\n",
    "    terrain_complexity = min(terrain_data['dtm'].std() / 20.0, 0.2)\n",
    "    \n",
    "    # åœ°ç†çš„ä½ç½®ã‚¹ã‚³ã‚¢ (0-0.1) - ã‚¢ãƒã‚¾ãƒ³ç›†åœ°ã®é‡è¦åœ°åŸŸ\n",
    "    location_score = 0.1  # ã‚¢ãƒã‚¾ãƒ³åœ°åŸŸã®ãŸã‚é«˜ã‚¹ã‚³ã‚¢\n",
    "    \n",
    "    total_score = structure_score + diversity_score + terrain_complexity + location_score\n",
    "    return min(total_score, 1.0)\n",
    "\n",
    "def determine_investigation_priority(score, structures):\n",
    "    \"\"\"\n",
    "    èª¿æŸ»å„ªå…ˆåº¦ã®æ±ºå®š\n",
    "    \"\"\"\n",
    "    total_structures = structures['summary']['total_structures']\n",
    "    \n",
    "    if score > 0.8 or total_structures > 30:\n",
    "        return \"ç·Šæ€¥ - å³åº§ã®è€ƒå¤å­¦èª¿æŸ»ãŒå¿…è¦\"\n",
    "    elif score > 0.6 or total_structures > 15:\n",
    "        return \"é«˜ - é‡è¦ãªè€ƒå¤å­¦çš„å¯èƒ½æ€§ã€èª¿æŸ»æ¨å¥¨\"\n",
    "    elif score > 0.4 or total_structures > 5:\n",
    "        return \"ä¸­ - é©åº¦ãªè€ƒå¤å­¦çš„é–¢å¿ƒã€è¿½åŠ è§£ææ¨å¥¨\"\n",
    "    else:\n",
    "        return \"ä½ - é™å®šçš„ãªè€ƒå¤å­¦æŒ‡æ¨™ã€ç›£è¦–æ¨å¥¨\"\n",
    "\n",
    "def generate_recommendations(score, structures, site_info):\n",
    "    \"\"\"\n",
    "    èª¿æŸ»æ¨å¥¨äº‹é …ã®ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    if score > 0.7:\n",
    "        recommendations.extend([\n",
    "            \"å³åº§ã®åœ°åŸŸè€ƒå¤å­¦å½“å±€ã¨ã®é€£æº\",\n",
    "            \"è­˜åˆ¥ã•ã‚ŒãŸæ§‹é€ ã®ç·Šæ€¥ä¿è­·æªç½®\",\n",
    "            \"åœ°ä¸­ãƒ¬ãƒ¼ãƒ€ãƒ¼èª¿æŸ»ã®è¿…é€Ÿãªå±•é–‹\",\n",
    "            \"åœ°åŸŸå…ˆä½æ°‘ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨ã®å”è­°\"\n",
    "        ])\n",
    "    else:\n",
    "        recommendations.extend([\n",
    "            \"è©³ç´°ãªåœ°ä¸­ãƒ¬ãƒ¼ãƒ€ãƒ¼èª¿æŸ»\",\n",
    "            \"æˆ¦ç•¥çš„è©¦æ˜ãƒ—ãƒ­ã‚°ãƒ©ãƒ \",\n",
    "            \"é«˜è§£åƒåº¦ãƒ‰ãƒ­ãƒ¼ãƒ³ãƒãƒƒãƒ”ãƒ³ã‚°\"\n",
    "        ])\n",
    "    \n",
    "    # æ§‹é€ åˆ¥æ¨å¥¨\n",
    "    if structures['summary']['circular_count'] > 2:\n",
    "        recommendations.append(\"å††å½¢æ§‹é€ ã®å„€å¼çš„æ„ç¾©ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸèª¿æŸ»\")\n",
    "    \n",
    "    if structures['summary']['linear_count'] > 3:\n",
    "        recommendations.append(\"ç·šå½¢æ§‹é€ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¤ä»£äº¤é€šã‚·ã‚¹ãƒ†ãƒ èª¿æŸ»\")\n",
    "    \n",
    "    if structures['summary']['mound_count'] > 5:\n",
    "        recommendations.append(\"ãƒã‚¦ãƒ³ãƒ‰ã®åŸ‹è‘¬ãƒ»å„€å¼æ©Ÿèƒ½ã«é–¢ã™ã‚‹å„ªå…ˆèª¿æŸ»\")\n",
    "    \n",
    "    # å…±é€šæ¨å¥¨\n",
    "    recommendations.extend([\n",
    "        \"çµ¶å¯¾å¹´ä»£æ¸¬å®šç”¨æ”¾å°„æ€§ç‚­ç´ ã‚µãƒ³ãƒ—ãƒ«æ¡å–\",\n",
    "        \"è¡¨é¢ç‰©è³ªå­˜åœ¨æ™‚ã®ã‚»ãƒ©ãƒŸãƒƒã‚¯åˆ†æ\",\n",
    "        \"å¤ç’°å¢ƒå¾©å…ƒã®ãŸã‚ã®ç’°å¢ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\",\n",
    "        \"ãƒ•ã‚©ãƒˆã‚°ãƒ©ãƒ¡ãƒˆãƒªãƒ¼ãƒ»3Dã‚¹ã‚­ãƒ£ãƒ³ã«ã‚ˆã‚‹è¨˜éŒ²\"\n",
    "    ])\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# ç·åˆè©•ä¾¡ã®å®Ÿè¡Œ\n",
    "archaeological_score = calculate_archaeological_score(detected_structures, terrain_data)\n",
    "investigation_priority = determine_investigation_priority(archaeological_score, detected_structures)\n",
    "recommendations = generate_recommendations(archaeological_score, detected_structures, site_info)\n",
    "\n",
    "print(\"\\n\" + \"ğŸ›ï¸\" + \"=\"*50)\n",
    "print(\"ç·åˆè€ƒå¤å­¦çš„è©•ä¾¡\")\n",
    "print(\"=\"*52)\n",
    "print(f\"ğŸ“Š è€ƒå¤å­¦çš„ã‚¹ã‚³ã‚¢: {archaeological_score:.3f}/1.000\")\n",
    "print(f\"âš ï¸ èª¿æŸ»å„ªå…ˆåº¦: {investigation_priority}\")\n",
    "print(f\"ğŸ” æ¤œå‡ºæ§‹é€ ç·æ•°: {detected_structures['summary']['total_structures']}\")\n",
    "print(\"\\nğŸ“‹ æ¨å¥¨äº‹é …:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "print(\"=\"*52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ çµæœã®ä¿å­˜ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(site_info, terrain_data, structures, \n",
    "                         archaeological_interpretation, archaeological_score, \n",
    "                         investigation_priority, recommendations):\n",
    "    \"\"\"\n",
    "    è§£æçµæœã®ä¿å­˜\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # ç·åˆçµæœè¾æ›¸\n",
    "    results = {\n",
    "        'metadata': {\n",
    "            'site_info': site_info,\n",
    "            'analysis_timestamp': timestamp,\n",
    "            'pipeline_version': '1.0'\n",
    "        },\n",
    "        'terrain_analysis': {\n",
    "            'elevation_stats': {\n",
    "                'min': float(terrain_data['dtm'].min()),\n",
    "                'max': float(terrain_data['dtm'].max()),\n",
    "                'mean': float(terrain_data['dtm'].mean()),\n",
    "                'std': float(terrain_data['dtm'].std())\n",
    "            },\n",
    "            'slope_stats': {\n",
    "                'mean': float(terrain_data['slope'].mean()),\n",
    "                'max': float(terrain_data['slope'].max())\n",
    "            }\n",
    "        },\n",
    "        'structure_detection': structures,\n",
    "        'archaeological_assessment': {\n",
    "            'overall_score': archaeological_score,\n",
    "            'investigation_priority': investigation_priority,\n",
    "            'llm_interpretation': archaeological_interpretation,\n",
    "            'recommendations': recommendations\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # JSONä¿å­˜\n",
    "    json_file = f'lidar_analysis_results_{timestamp}.json'\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # CSVä¿å­˜ï¼ˆæ§‹é€ ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "    structure_data = []\n",
    "    for structure_type in ['circular_features', 'linear_features', 'mounds', 'depressions']:\n",
    "        for i, structure in enumerate(structures[structure_type]):\n",
    "            row = {\n",
    "                'id': i,\n",
    "                'type': structure_type,\n",
    "                'confidence': structure.get('confidence', 0.0)\n",
    "            }\n",
    "            \n",
    "            if 'center' in structure:\n",
    "                # ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã‚’åœ°ç†åº§æ¨™ã«æ¦‚ç®—å¤‰æ›\n",
    "                pixel_to_geo_x = site_info['coordinates'][1] + (structure['center'][1] / 512) * 0.01\n",
    "                pixel_to_geo_y = site_info['coordinates'][0] + (structure['center'][0] / 512) * 0.01\n",
    "                row['longitude'] = pixel_to_geo_x\n",
    "                row['latitude'] = pixel_to_geo_y\n",
    "            \n",
    "            # æ§‹é€ å›ºæœ‰å±æ€§\n",
    "            for key in ['radius', 'height', 'depth', 'length', 'angle']:\n",
    "                if key in structure:\n",
    "                    row[key] = structure[key]\n",
    "            \n",
    "            structure_data.append(row)\n",
    "    \n",
    "    csv_file = f'archaeological_structures_{timestamp}.csv'\n",
    "    df = pd.DataFrame(structure_data)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜\n",
    "    report_file = f'archaeological_report_{timestamp}.md'\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# LIDARè€ƒå¤å­¦è§£æãƒ¬ãƒãƒ¼ãƒˆ\\n\\n\")\n",
    "        f.write(f\"**ã‚µã‚¤ãƒˆ**: {site_info['name']}\\n\")\n",
    "        f.write(f\"**åº§æ¨™**: {site_info['coordinates']}\\n\")\n",
    "        f.write(f\"**è§£ææ—¥**: {timestamp}\\n\\n\")\n",
    "        f.write(f\"## ç·åˆè©•ä¾¡\\n\")\n",
    "        f.write(f\"- **è€ƒå¤å­¦çš„ã‚¹ã‚³ã‚¢**: {archaeological_score:.3f}/1.000\\n\")\n",
    "        f.write(f\"- **èª¿æŸ»å„ªå…ˆåº¦**: {investigation_priority}\\n\")\n",
    "        f.write(f\"- **æ¤œå‡ºæ§‹é€ æ•°**: {structures['summary']['total_structures']}\\n\\n\")\n",
    "        f.write(f\"## LLMè§£æ\\n\")\n",
    "        f.write(archaeological_interpretation)\n",
    "        f.write(f\"\\n\\n## æ¨å¥¨äº‹é …\\n\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            f.write(f\"{i}. {rec}\\n\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ çµæœä¿å­˜å®Œäº†:\")\n",
    "    print(f\"   ğŸ“„ JSON: {json_file}\")\n",
    "    print(f\"   ğŸ“Š CSV: {csv_file}\")\n",
    "    print(f\"   ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}\")\n",
    "    \n",
    "    return {\n",
    "        'json_file': json_file,\n",
    "        'csv_file': csv_file,\n",
    "        'report_file': report_file\n",
    "    }\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "saved_files = save_analysis_results(\n",
    "    site_info, terrain_data, detected_structures,\n",
    "    archaeological_interpretation, archaeological_score,\n",
    "    investigation_priority, recommendations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚çµæœã®è¡¨ç¤º\n",
    "print(\"\\n\" + \"ğŸ¯\" + \"=\"*60)\n",
    "print(\"LIDARè€ƒå¤å­¦è§£æ - æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼\")\n",
    "print(\"=\"*63)\n",
    "print(f\"ğŸ“ ã‚µã‚¤ãƒˆ: {site_info['name']}\")\n",
    "print(f\"ğŸŒ ä½ç½®: {site_info['coordinates'][0]:.3f}, {site_info['coordinates'][1]:.3f}\")\n",
    "print(f\"ğŸ“… è§£ææ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nğŸ›ï¸ æ¤œå‡ºçµæœ:\")\n",
    "print(f\"   ç·æ§‹é€ æ•°: {detected_structures['summary']['total_structures']}\")\n",
    "print(f\"   - å††å½¢æ§‹é€ : {detected_structures['summary']['circular_count']}å€‹\")\n",
    "print(f\"   - ç·šå½¢æ§‹é€ : {detected_structures['summary']['linear_count']}å€‹\")\n",
    "print(f\"   - ãƒã‚¦ãƒ³ãƒ‰: {detected_structures['summary']['mound_count']}å€‹\")\n",
    "print(f\"   - çªªåœ°: {detected_structures['summary']['depression_count']}å€‹\")\n",
    "print(f\"\\nğŸ“Š è©•ä¾¡:\")\n",
    "print(f\"   è€ƒå¤å­¦çš„ã‚¹ã‚³ã‚¢: {archaeological_score:.3f}/1.000\")\n",
    "print(f\"   èª¿æŸ»å„ªå…ˆåº¦: {investigation_priority}\")\n",
    "print(f\"\\nğŸ’¾ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "for file_type, filename in saved_files.items():\n",
    "    print(f\"   {file_type}: {filename}\")\n",
    "print(\"\\nğŸ”¬ æ¨å¥¨æ¬¡ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "for i, rec in enumerate(recommendations[:5], 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "print(\"\\nâœ… è§£æå®Œäº†\")\n",
    "print(\"=\"*63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ä½¿ç”¨æ–¹æ³•ã¨æ‹¡å¼µ\n",
    "\n",
    "### ã“ã®Notebookã®ä½¿ç”¨æ–¹æ³•:\n",
    "1. **å®Ÿéš›ã®LIDARãƒ‡ãƒ¼ã‚¿ä½¿ç”¨**: `create_synthetic_lidar_data()` ã‚’å®Ÿéš›ã®.las/.lazãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«ç½®ãæ›ãˆ\n",
    "2. **LLM APIè¨­å®š**: OpenAI/OpenRouter APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š\n",
    "3. **GISå‡ºåŠ›**: rasterio/geopandasã§Shapefileç­‰ã®å‡ºåŠ›æ©Ÿèƒ½è¿½åŠ \n",
    "\n",
    "### æ‹¡å¼µå¯èƒ½ãªæ©Ÿèƒ½:\n",
    "- **æ·±å±¤å­¦ç¿’**: PyTorchã§CNN/U-Netæ§‹é€ æ¤œå‡ºãƒ¢ãƒ‡ãƒ«\n",
    "- **3Då¯è¦–åŒ–**: Open3D/Plotlyã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–Viewer\n",
    "- **æ™‚ç³»åˆ—è§£æ**: è¤‡æ•°æ™‚æœŸã®LIDARãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒ\n",
    "- **WebGIS**: Folium/Leafletã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ—\n",
    "\n",
    "### APIè¨­å®šä¾‹:\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"your-openrouter-key\"\n",
    "export OPENAI_BASE_URL=\"https://openrouter.ai/api/v1\"\n",
    "export OPENAI_MODEL=\"deepseek/deepseek-r1-0528:free\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101597,"databundleVersionId":12334818,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":59.620148,"end_time":"2025-06-17T15:34:53.608644","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-17T15:33:53.988496","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0513952586bb40c8b4f0322c4c6812ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"0f6209b859994d0fb324a7067dac03ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_4ba1ad9c5f8a41fea649b021c2e30a48","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d1879672ba54bbd95609514c39ccd3e","tabbable":null,"tooltip":null,"value":4}},"120ef0477b214200b3b8e8c9ae030608":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1585b25e60ec471b84ac68d52743822c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15febf76e8144841936c4d13820a7baf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"188d9446d95a4d21b950071dd2283965":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5899e3fd08ca4ce7beb7797bf99c5fec","IPY_MODEL_8845d45a862f4c85896021f3b0f23447","IPY_MODEL_6d4052cb98e24b80939cd12a1bc56c46"],"layout":"IPY_MODEL_15febf76e8144841936c4d13820a7baf","tabbable":null,"tooltip":null}},"194e9e5ac0d54ad2b586abc26ca3984a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"1c422f1dd1124c2ebb9aa14c7a6264e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_120ef0477b214200b3b8e8c9ae030608","placeholder":"​","style":"IPY_MODEL_1fbd26bc405a4b13a6a21b02ebd51f75","tabbable":null,"tooltip":null,"value":" 5/5 [00:11&lt;00:00,  2.29s/it]"}},"1d1879672ba54bbd95609514c39ccd3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1fbd26bc405a4b13a6a21b02ebd51f75":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2e0cb54c0fc9455c9fa8ac69070dc268":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc7f5d8b021f4e56b7f5c0220b87e5ee","IPY_MODEL_8bb3e6a0ab7c4b3fbc3e600bcea8f030","IPY_MODEL_1c422f1dd1124c2ebb9aa14c7a6264e7"],"layout":"IPY_MODEL_e94b3243ac534b1f9cb9ea766e93ce18","tabbable":null,"tooltip":null}},"4ba1ad9c5f8a41fea649b021c2e30a48":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5161624e1d844fbb9909be09d47e51b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e79e3a851e484ea39c71a21d9a2c6803","IPY_MODEL_0f6209b859994d0fb324a7067dac03ff","IPY_MODEL_596985ec521645d6a4e2a356b543bbe0"],"layout":"IPY_MODEL_83ed12cf62844330bb155f2556ae9a35","tabbable":null,"tooltip":null}},"564c72dea2b2471eb6ce68e904b15ade":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5899e3fd08ca4ce7beb7797bf99c5fec":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_564c72dea2b2471eb6ce68e904b15ade","placeholder":"​","style":"IPY_MODEL_60cd28dd1ed4437099993aca3b3df9eb","tabbable":null,"tooltip":null,"value":"Analyzing Tiles: 100%"}},"596985ec521645d6a4e2a356b543bbe0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_dedc4be99a7a460d9c3d91bdf3b8f166","placeholder":"​","style":"IPY_MODEL_0513952586bb40c8b4f0322c4c6812ce","tabbable":null,"tooltip":null,"value":" 4/4 [00:00&lt;00:00,  6.50it/s]"}},"60cd28dd1ed4437099993aca3b3df9eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6d4052cb98e24b80939cd12a1bc56c46":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_bd7d0bee6ca146fb9d04a3ddf4c060db","placeholder":"​","style":"IPY_MODEL_d9d75c5b76894414b343e6cb6d1011fc","tabbable":null,"tooltip":null,"value":" 16/16 [00:00&lt;00:00, 84.24it/s]"}},"83ed12cf62844330bb155f2556ae9a35":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8845d45a862f4c85896021f3b0f23447":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_f050cc7261dc4551a7a4384cd49353b2","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e99f67cab17743baa9393192ae7302e1","tabbable":null,"tooltip":null,"value":16}},"8bb3e6a0ab7c4b3fbc3e600bcea8f030":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_c1ad83ad24be4f14b716021f0e60ba22","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97b023dc51174ac9804eeac69648ba4f","tabbable":null,"tooltip":null,"value":5}},"97b023dc51174ac9804eeac69648ba4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd7d0bee6ca146fb9d04a3ddf4c060db":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1ad83ad24be4f14b716021f0e60ba22":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8b8b827f9a04793a7e15edc7d5324ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"cc7f5d8b021f4e56b7f5c0220b87e5ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fe2c1c747af94a70b59019b20d4382f8","placeholder":"​","style":"IPY_MODEL_c8b8b827f9a04793a7e15edc7d5324ae","tabbable":null,"tooltip":null,"value":"LLM Analysis: 100%"}},"d9d75c5b76894414b343e6cb6d1011fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"dedc4be99a7a460d9c3d91bdf3b8f166":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e79e3a851e484ea39c71a21d9a2c6803":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1585b25e60ec471b84ac68d52743822c","placeholder":"​","style":"IPY_MODEL_194e9e5ac0d54ad2b586abc26ca3984a","tabbable":null,"tooltip":null,"value":"Tiling Progress: 100%"}},"e94b3243ac534b1f9cb9ea766e93ce18":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e99f67cab17743baa9393192ae7302e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f050cc7261dc4551a7a4384cd49353b2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe2c1c747af94a70b59019b20d4382f8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"1e336f1f","cell_type":"markdown","source":"<div style=\"\n  background-image: linear-gradient(to right, #1E824C, #A9DFBF, #1E824C);\n  border: 2px solid #004D40;\n  border-radius: 10px;\n  padding: 20px;\n  text-align: center;\n  color: #F5FFFA;\n\">\n  <img src=\"https://www.kaggle.com/competitions/101597/images/header\" alt=\"Logos\" style=\"width: 250px; margin-bottom: 10px;\">\n  <h1 style=\"font-size: 2.2em; font-weight: bold; margin: 10px 0; text-shadow: 1px 1px 3px #004D40;\">Amazonia-AI: A Hybrid CV + LLM Approach</h1>\n  <p style=\"font-size: 1.2em; color: #E8F8F5;\">An AI-Assisted Archaeological Exploration</p>\n</div>\n\n# Amazonia-AI: Hybrid CV + LLM for Lost City Discovery\n\n**Author:** Zdanovic Konstantin\n<br>\n**Competition:** [OpenAI to Z Challenge](https://www.kaggle.com/competitions/openai-to-z-challenge)\n\n---\n\n### **Table of Contents**\n\n1.  [**Introduction: Peering Through the Canopy**](#introduction)\n2.  [**Methodology & Core Principles**](#methodology)\n3.  [**Setup: Dependencies and Configuration**](#setup)\n4.  [**Stage 1: Data Pipeline & Preprocessing**](#stage1)\n5.  [**Stage 2: Candidate Generation with Classical CV**](#stage2)\n6.  [**Stage 3: Deep Analysis with GPT-4o**](#stage3)\n7.  [**Stage 4: Results Visualization & Interpretation**](#stage4)\n8.  [**Conclusion & Next Steps**](#conclusion)\n\n<a id='introduction'></a>\n\n## 1. Introduction: Peering Through the Canopy\n\nThe Amazon Basin, the world's largest tropical rainforest, is a realm of staggering biodiversity and profound historical mystery. For centuries, it has been speculated that large, complex societies thrived within its depths long before European contact. Yet, the dense, triple-canopy jungle has acted as a formidable veil, concealing the secrets of these past civilizations. Traditional archaeological methods are slow, expensive, and often impractical across such a vast and inhospitable terrain.\n\n**The Challenge:** How can we systematically and efficiently survey millions of square kilometers of inaccessible jungle to find faint, earth-toned signals of ancient human activity?\n\n**Our Goal:** This notebook presents a **hybrid, two-stage intelligence pipeline** designed to tackle this very problem. We combine the scalability of classical Computer Vision (CV) and Geographic Information Systems (GIS) with the nuanced, contextual understanding of a state-of-the-art Multimodal Large Language Model (LLM). Our mission is to build a scalable and cost-effective system to identify high-potential candidate sites for previously unknown archaeological features, such as:\n\n- *Geoglyphs* (large geometric earthworks)\n- *Ancient settlements* (indicated by soil changes, causeways, or mounds)\n- *Terraced agricultural systems*\n\nThis project directly addresses the core objective of the **OpenAI to Z Challenge**: to leverage OpenAI's powerful models to push the boundaries of scientific discovery. By developing an explainable, reproducible, and efficient workflow, we aim to provide a benchmark for AI-assisted archaeology.","metadata":{"papermill":{"duration":0.005308,"end_time":"2025-06-17T15:33:59.598972","exception":false,"start_time":"2025-06-17T15:33:59.593664","status":"completed"},"tags":[]}},{"id":"299d8757","cell_type":"markdown","source":"<a id='methodology'></a>\n## 2. Methodology & Core Principles\n\nOur approach is founded on four guiding principles, ensuring our methodology is robust, transparent, and practical.\n\n<div style=\"border: 2px solid #4CAF50; border-radius: 8px; padding: 15px; margin-bottom: 15px; background-color: #1a2c1a;\">\n    <h4>Principle 1: Scientific Rigor and Explainability</h4>\n    <p>We reject \"black-box\" solutions. Every stage of our pipeline is designed to be interpretable. The initial filtering relies on well-understood CV heuristics (e.g., edge and shape detection), and the final LLM analysis is prompted to provide a detailed rationale for its conclusions. This allows archaeologists to understand <em>why</em> a site was flagged, building trust and facilitating further investigation.</p>\n</div>\n\n<div style=\"border: 2px solid #2196F3; border-radius: 8px; padding: 15px; margin-bottom: 15px; background-color: #15222e;\">\n    <h4>Principle 2: Hybrid Multimodal Pipeline</h4>\n    <p>A hybrid model is superior to a purely LLM-based approach. Relying solely on a powerful LLM to scan the entire Amazon would be computationally and financially prohibitive. Our two-stage system is smarter:</p>\n    <ul>\n        <li><strong>Stage 1 (Broad Filtering):</strong> Use inexpensive, classical GIS and CV algorithms to analyze 100% of the area and automatically discard >99% of uninteresting terrain.</li>\n        <li><strong>Stage 2 (Deep Analysis):</strong> Use the powerful (but costly) GPT-4o model to perform a detailed, expert-level analysis on the remaining <1% of high-potential candidates.</li>\n    </ul>\n</div>\n\n<div style=\"border: 2px solid #ff9800; border-radius: 8px; padding: 15px; margin-bottom: 15px; background-color: #2e2413;\">\n    <h4>Principle 3: Iterative & Reproducible</h4>\n    <p>This notebook is a self-contained research object. The code is modular, well-documented, and parameterized through a central <code>CONFIG</code> object. This design allows for easy reproduction of our results and encourages extension. Other researchers can easily swap in different models, new filtering algorithms, or target different Areas of Interest (AOIs).</p>\n</div>\n\n<div style=\"border: 2px solid #f44336; border-radius: 8px; padding: 15px; margin-bottom: 15px; background-color: #2c1a1a;\">\n    <h4>Principle 4: Cost-Efficiency and Scalability</h4>\n    <p>A key design constraint is minimizing API costs. Our pre-filtering stage ensures that we only spend our <code>OpenAI API</code> budget on the most promising data points. We also implement local caching to avoid redundant computations. The entire pipeline is built with scalability in mind, capable of processing vast geographical areas.</p>\n</div>","metadata":{"papermill":{"duration":0.003905,"end_time":"2025-06-17T15:33:59.607469","exception":false,"start_time":"2025-06-17T15:33:59.603564","status":"completed"},"tags":[]}},{"id":"d18c7e1e","cell_type":"markdown","source":"<a id='setup'></a>\n## 3. Setup: Dependencies and Configuration\n\nLet's begin by setting up our environment. This cell installs all necessary libraries for geospatial analysis, computer vision, and interaction with OpenAI's API.","metadata":{"papermill":{"duration":0.003966,"end_time":"2025-06-17T15:33:59.61568","exception":false,"start_time":"2025-06-17T15:33:59.611714","status":"completed"},"tags":[]}},{"id":"d790e9c8","cell_type":"code","source":"%%capture\n!pip install -q geopandas rasterio folium opencv-python-headless scikit-image openai google-api-python-client earthengine-api tqdm","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:22.62348Z","iopub.execute_input":"2025-06-17T16:20:22.62381Z","iopub.status.idle":"2025-06-17T16:20:46.821415Z","shell.execute_reply.started":"2025-06-17T16:20:22.623784Z","shell.execute_reply":"2025-06-17T16:20:46.820305Z"},"papermill":{"duration":28.721225,"end_time":"2025-06-17T15:34:28.341177","exception":false,"start_time":"2025-06-17T15:33:59.619952","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"cd73ce28","cell_type":"code","source":"# Core Python libraries\nimport os\nimport json\nimport time\nimport base64\nimport warnings\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\n\n# Geospatial and Data Handling\nimport pandas as pd\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nfrom rasterio.transform import from_origin\nimport ee # Google Earth Engine\nimport folium\nfrom folium.plugins import MarkerCluster\n\n# Image Processing and Visualization\nimport cv2\nimport matplotlib.pyplot as plt\nimport folium # For interactive maps\n\n# APIs and Integration\nimport openai\n\n# Environment setup\nIS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\nwarnings.filterwarnings('ignore')\n\nprint(f\"Setup Complete. Running in {'Kaggle' if IS_KAGGLE else 'Local'} environment.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:46.823192Z","iopub.execute_input":"2025-06-17T16:20:46.823536Z","iopub.status.idle":"2025-06-17T16:20:52.444933Z","shell.execute_reply.started":"2025-06-17T16:20:46.823481Z","shell.execute_reply":"2025-06-17T16:20:52.443973Z"},"papermill":{"duration":7.663992,"end_time":"2025-06-17T15:34:36.00967","exception":false,"start_time":"2025-06-17T15:34:28.345678","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"9779d59d","cell_type":"code","source":"# --- 1. Load Secrets ---\nif IS_KAGGLE:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        OPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\n        print(\"Kaggle secrets loaded successfully.\")\n    except Exception as e:\n        print(f\"Could not load Kaggle secrets: {e}. Please ensure OPENAI_API_KEY is set.\")\n        OPENAI_API_KEY = \"DUMMY_KEY\"\nelse:\n    from dotenv import load_dotenv\n    load_dotenv()\n    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n    openai.api_key = OPENAI_API_KEY\n    print(\"Local .env secrets loaded.\")\n\n# --- 2. Define Core Configuration ---\nCONFIG = {\n    'USE_MOCK_DATA': True,\n    \n    # Area of Interest: A region in Acre, Brazil known for geoglyphs\n    'AOI_BOUNDS': {'west':-70.8, 'south':-10.5, 'east':-69.8, 'north':-9.5}, \n    \n    # Tiling parameters\n    'TILE_SIZE_PX': 512, \n    'TILE_OVERLAP_PX': 64,\n    \n    # GEE parameters\n    'DATE_RANGE': ('2023-06-01', '2023-09-30'),\n    'CLOUDY_PIXEL_PERCENTAGE': 10,\n\n    # CV filter threshold\n    'GEOMETRY_SCORE_THRESHOLD': 5,\n    'VISUALIZATION_SAMPLES': 3,\n    'LLM_ANALYSIS_COUNT': 5,\n\n    # Paths\n    'BASE_DIR': Path('./amazonia_ai'),\n    'SOURCE_DATA_DIR': Path('./amazonia_ai/01_source_data'),\n    'TILES_DIR': Path('./amazonia_ai/02_tiles'),\n    'RESULTS_DIR': Path('./amazonia_ai/03_results'),\n    'CACHE_DIR': Path('./amazonia_ai/cache')\n}\n\nfor p in ['BASE_DIR', 'SOURCE_DATA_DIR', 'TILES_DIR', 'RESULTS_DIR', 'CACHE_DIR']:\n    CONFIG[p].mkdir(exist_ok=True)\nprint(\"Project directory structure verified.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:52.445786Z","iopub.execute_input":"2025-06-17T16:20:52.446329Z","iopub.status.idle":"2025-06-17T16:20:52.559312Z","shell.execute_reply.started":"2025-06-17T16:20:52.446301Z","shell.execute_reply":"2025-06-17T16:20:52.558512Z"},"papermill":{"duration":0.164741,"end_time":"2025-06-17T15:34:36.178939","exception":false,"start_time":"2025-06-17T15:34:36.014198","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"df496ef5","cell_type":"markdown","source":"<a id='stage1'></a>\n## STAGE 1: Data Pipeline & Preprocessing\n\nOur first task is to acquire the raw data. Ideally, we would use Google Earth Engine (GEE) to access two primary data sources:\n\n1.  **Sentinel-2 Satellite Imagery:** This provides us with recent, high-resolution (10m) multispectral data. We use the Red, Green, and Blue bands for true-color images, and the Near-Infrared (NIR) band to calculate the Normalized Difference Vegetation Index (NDVI), which is excellent for highlighting changes in vegetation and soil—a key indicator of human activity.\n2.  **SRTM Digital Elevation Model (DEM):** The Shuttle Radar Topography Mission gives us elevation data. We use this to create a slope map, which can reveal subtle earthworks like mounds, moats, and causeways that are invisible in standard imagery.\n\nFor reproducibility on Kaggle without requiring GEE authentication, this notebook will run in `MOCK_DATA` mode by default. In this mode, we programmatically generate a mock raster file that simulates an Amazonian landscape with embedded geometric anomalies. The real GEE pipeline logic is included for reference.\n\nAfter sourcing the data (real or mock), we tile the large raster into smaller, manageable `512x512` pixel chips for efficient processing.","metadata":{"papermill":{"duration":0.004148,"end_time":"2025-06-17T15:34:36.187618","exception":false,"start_time":"2025-06-17T15:34:36.18347","status":"completed"},"tags":[]}},{"id":"d6fcd3fa","cell_type":"code","source":"# === MOCK DATA GENERATION ===\ndef create_mock_raster(config):\n    \"\"\"Creates a mock raster file with more 'natural' simulated anomalies.\"\"\"\n    print(\"--- Creating MOCK Data File ---\")\n    mock_path = config['SOURCE_DATA_DIR'] / 'MOCK_DATA.tif'\n    shape = (2048, 2048)\n\n    background = np.random.normal(80, 20, (shape[0], shape[1])).astype(np.uint8)\n    background = cv2.GaussianBlur(background, (9, 9), 0)\n\n    anomalies_layer = np.zeros(shape, dtype=np.uint8)\n    cv2.rectangle(anomalies_layer, (200, 200), (500, 500), 200, -1)\n    # Круг\n    cv2.circle(anomalies_layer, (1300, 1300), 150, 180, -1)\n    cv2.line(anomalies_layer, (800, 100), (800, 600), 190, 8)\n    cv2.line(anomalies_layer, (700, 400), (950, 400), 190, 8)\n    \n    anomalies_layer = cv2.GaussianBlur(anomalies_layer, (11, 11), 0)\n    \n    final_raster = cv2.addWeighted(background, 0.7, anomalies_layer, 0.9, 0)\n    \n    aoi_bounds = config['AOI_BOUNDS']\n    transform = from_origin(aoi_bounds['west'], aoi_bounds['north'], 0.0001, 0.0001)\n    profile = {\n        'driver': 'GTiff', 'count': 5, 'dtype': 'uint8',\n        'width': shape[1], 'height': shape[0],\n        'crs': 'EPSG:4326', 'transform': transform\n    }\n    with rasterio.open(mock_path, 'w', **profile) as dst:\n        for i in range(1, 6):\n            dst.write(final_raster, i)\n            \n    print(f\"Mock data saved to {mock_path}\")\n    return mock_path\n\n# === TILING LOGIC ===\ndef process_and_tile_data(source_path, config):\n    \"\"\"Tiles a source raster into smaller overlapping chips for analysis.\"\"\"\n    print(f\"--- Tiling data from {source_path} ---\")\n    metadata, tile_coords = [], []\n    tile_size = config['TILE_SIZE_PX']\n    step = config['TILE_SIZE_PX'] - config['TILE_OVERLAP_PX']\n    \n    with rasterio.open(source_path) as src:\n        for y in tqdm(range(0, src.height - tile_size, step), desc=\"Tiling Progress\"):\n            for x in range(0, src.width - tile_size, step):\n                window = Window(x, y, tile_size, tile_size)\n                tile_dir = config['TILES_DIR'] / f\"tile_{x}_{y}\"\n                tile_dir.mkdir(exist_ok=True)\n                \n                rgb_path = tile_dir / \"rgb.png\"\n                ndvi_path = tile_dir / \"ndvi.tif\"\n                slope_path = tile_dir / \"slope.tif\"\n                \n                bands = src.read(window=window)\n                \n                rgb_bands = bands[:3]\n                v_min, v_max = np.percentile(rgb_bands, [2, 98])\n                rgb_stretched = np.clip((rgb_bands - v_min) * 255.0 / (v_max - v_min), 0, 255).astype(np.uint8)\n                rgb_img = np.dstack(rgb_stretched)\n                cv2.imwrite(str(rgb_path), cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR))\n\n                base_profile = src.profile.copy()\n                base_profile.update(width=tile_size, height=tile_size, transform=src.window_transform(window), count=1, dtype='uint8')\n                \n                with rasterio.open(ndvi_path, 'w', **base_profile) as dst: dst.write(bands[3], 1)\n                with rasterio.open(slope_path, 'w', **base_profile) as dst: dst.write(bands[4], 1)\n                \n                coords = src.xy(y + tile_size // 2, x + tile_size // 2)\n                metadata.append({\n                    'tile_id': f\"tile_{x}_{y}\", 'rgb_path': str(rgb_path), 'ndvi_path': str(ndvi_path), 'slope_path': str(slope_path),\n                    'lon': coords[0], 'lat': coords[1]\n                })\n    \n    df = pd.DataFrame(metadata)\n    print(f\"Tiling complete. {len(df)} tiles created.\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:52.560975Z","iopub.execute_input":"2025-06-17T16:20:52.561243Z","iopub.status.idle":"2025-06-17T16:20:52.576081Z","shell.execute_reply.started":"2025-06-17T16:20:52.561221Z","shell.execute_reply":"2025-06-17T16:20:52.575103Z"},"papermill":{"duration":0.024473,"end_time":"2025-06-17T15:34:36.216491","exception":false,"start_time":"2025-06-17T15:34:36.192018","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"bd3cd02f","cell_type":"code","source":"# === DATA PIPELINE EXECUTION ===\ndf_tiles = pd.DataFrame()\n\nif CONFIG['USE_MOCK_DATA']:\n    print(\"INFO: Using MOCK data pipeline.\")\n    mock_file_path = create_mock_raster(CONFIG)\n    df_tiles = process_and_tile_data(mock_file_path, CONFIG)\nelse:\n    print(\"INFO: REAL data pipeline would run here.\")\n    print(\"This requires GEE authentication and Google Drive setup.\")\n    print(\"To run, implement the GEE download logic and set USE_MOCK_DATA to False.\")\n\nif not df_tiles.empty:\n    print(\"\\n--- Sample Generated Tiles ---\")\n    sample_rows = df_tiles.sample(min(CONFIG['VISUALIZATION_SAMPLES'], len(df_tiles)))\n    for _, row in sample_rows.iterrows():\n        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n        img = cv2.imread(row['rgb_path'])\n        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        ax.set_title(f\"Tile: {row['tile_id']}\")\n        ax.axis('off')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:52.576866Z","iopub.execute_input":"2025-06-17T16:20:52.577134Z","iopub.status.idle":"2025-06-17T16:20:54.150214Z","shell.execute_reply.started":"2025-06-17T16:20:52.577113Z","shell.execute_reply":"2025-06-17T16:20:54.149232Z"},"papermill":{"duration":1.347373,"end_time":"2025-06-17T15:34:37.569119","exception":false,"start_time":"2025-06-17T15:34:36.221746","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"56317de5","cell_type":"markdown","source":"<a id='stage2'></a>\n## STAGE 2: Candidate Generation with Classical CV\n\nThis is the core of our cost-saving strategy. Instead of sending thousands of tiles to the OpenAI API, we apply a series of fast, inexpensive, and interpretable filters to identify tiles that exhibit *anomalous* characteristics. We are looking for patterns that are unlikely to occur in nature, such as:\n\n- **Geometric Regularity:** Perfectly straight lines, right angles, and circular or rectangular shapes are strong indicators of human construction. We use a combination of Canny edge detection and Hough line transforms to score tiles based on their geometric structure.\n- **Vegetation Anomalies (Future Work):** Ancient settlements and agriculture can alter soil composition, leading to distinct patterns in vegetation that persist for centuries. We would analyze the NDVI tiles, looking for large, unusually shaped contours that differ from the surrounding jungle. For this notebook, we focus on geometric detection.","metadata":{"papermill":{"duration":0.007852,"end_time":"2025-06-17T15:34:37.58623","exception":false,"start_time":"2025-06-17T15:34:37.578378","status":"completed"},"tags":[]}},{"id":"2c7c4bbd","cell_type":"code","source":"# === CV FILTERING FUNCTIONS ===\ndef detect_geometric_shapes(image_path):\n    \"\"\"Applies Canny edge detection and Hough line transform to find straight lines.\"\"\"\n    try:\n        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n        if img is None: return 0, None, None\n        \n        edges = cv2.Canny(img, threshold1=30, threshold2=150)\n        \n        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=25, minLineLength=30, maxLineGap=10)\n        \n        viz_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n        num_lines = 0\n        if lines is not None:\n            num_lines = len(lines)\n            for line in lines:\n                x1, y1, x2, y2 = line[0]\n                cv2.line(viz_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                \n        return num_lines, edges, viz_img\n    except Exception as e:\n        print(f\"CV processing error for {image_path}: {e}\")\n        return 0, None, None","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:54.151333Z","iopub.execute_input":"2025-06-17T16:20:54.151683Z","iopub.status.idle":"2025-06-17T16:20:54.159873Z","shell.execute_reply.started":"2025-06-17T16:20:54.151654Z","shell.execute_reply":"2025-06-17T16:20:54.1587Z"},"papermill":{"duration":0.019816,"end_time":"2025-06-17T15:34:37.614512","exception":false,"start_time":"2025-06-17T15:34:37.594696","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"334b52a0","cell_type":"code","source":"# === CV ANALYSIS EXECUTION ===\ndf_analysis = pd.DataFrame()\ndf_candidates = pd.DataFrame()\n\nif not df_tiles.empty:\n    print(\"--- Filtering All Tiles with Classical CV ---\")\n    analysis_results = []\n    for _, row in tqdm(df_tiles.iterrows(), total=len(df_tiles), desc=\"Analyzing Tiles\"):\n        geometry_score, viz_canny, viz_geometry = detect_geometric_shapes(row['rgb_path'])\n        \n        result_row = row.to_dict()\n        result_row.update({\n            'geometry_score': geometry_score,\n            'is_candidate': geometry_score >= CONFIG['GEOMETRY_SCORE_THRESHOLD']\n        })\n        analysis_results.append(result_row)\n        \n    df_analysis = pd.DataFrame(analysis_results)\n    df_candidates = df_analysis[df_analysis['is_candidate']].copy()\n    \n    print(f\"\\nFiltering complete. Found {len(df_candidates)} high-potential candidates out of {len(df_analysis)} total tiles.\")\nelse:\n    print(\"Skipping CV filtering as no tiles were loaded.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:54.160952Z","iopub.execute_input":"2025-06-17T16:20:54.161296Z","iopub.status.idle":"2025-06-17T16:20:54.9944Z","shell.execute_reply.started":"2025-06-17T16:20:54.161265Z","shell.execute_reply":"2025-06-17T16:20:54.993217Z"},"papermill":{"duration":0.200643,"end_time":"2025-06-17T15:34:37.823422","exception":false,"start_time":"2025-06-17T15:34:37.622779","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"b5540d76","cell_type":"code","source":"# === VISUALIZATION OF CV FILTERING RESULTS ===\ndef visualize_cv_steps(df_analysis, num_to_show=2):\n    \"\"\"Displays a detailed, stylish comparison of tiles that passed and failed the CV filter.\"\"\"\n    plt.style.use('dark_background')\n    for state, color, df_subset in [('CANDIDATE (PASSED)', '#4CAF50', df_analysis[df_analysis['is_candidate']]), \n                                ('REJECTED', '#f44336', df_analysis[~df_analysis['is_candidate']])]:\n        if not df_subset.empty:\n            print(f\"\\n--- Showing {min(num_to_show, len(df_subset))} examples for: {state} ---\")\n            for _, row in df_subset.head(num_to_show).iterrows():\n                _, viz_canny, viz_geometry = detect_geometric_shapes(row['rgb_path'])\n                fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n                fig.patch.set_facecolor('#1a1a1a')\n                \n                axes[0].imshow(cv2.cvtColor(cv2.imread(row['rgb_path']), cv2.COLOR_BGR2RGB))\n                axes[0].set_title(\"Original RGB Tile\")\n                axes[1].imshow(viz_canny, cmap='hot')\n                axes[1].set_title(\"Step 1: Canny Edges\")\n                axes[2].imshow(cv2.cvtColor(viz_geometry, cv2.COLOR_BGR2RGB))\n                axes[2].set_title(f\"Step 2: Hough Lines (Score: {row['geometry_score']})\")\n                \n                fig.suptitle(f\"CV Analysis of Tile: {row['tile_id']} -> {state}\", fontsize=16, weight='bold', color=color)\n                for ax in axes: ax.axis('off')\n                plt.tight_layout(rect=[0, 0, 1, 0.94])\n                plt.show()\n\nif not df_analysis.empty:\n    candidates_path = CONFIG['RESULTS_DIR'] / 'cv_candidates.csv'\n    df_candidates.to_csv(candidates_path, index=False)\n    print(f\"Candidate list saved to: {candidates_path}\")\n    visualize_cv_steps(df_analysis, num_to_show=CONFIG['VISUALIZATION_SAMPLES'])\nelse:\n    print(\"No CV analysis results to visualize.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:54.99565Z","iopub.execute_input":"2025-06-17T16:20:54.996015Z","iopub.status.idle":"2025-06-17T16:20:56.984957Z","shell.execute_reply.started":"2025-06-17T16:20:54.995982Z","shell.execute_reply":"2025-06-17T16:20:56.98386Z"},"papermill":{"duration":2.659911,"end_time":"2025-06-17T15:34:40.491857","exception":false,"start_time":"2025-06-17T15:34:37.831946","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"addf1c92","cell_type":"markdown","source":"<a id='stage3'></a>\n## STAGE 3: Deep Analysis with GPT-4o Multimodal LLM\n\nNow that we have a small, manageable list of high-potential candidates, we can leverage the sophisticated reasoning power of **GPT-4o**. For each candidate, we create a **composite image**. This image places the true-color RGB tile side-by-side with visualized representations of the NDVI map (vegetation) and the slope map (topography). This gives the model maximum visual context to make an informed judgment.\n\nWe then query the model with a carefully crafted prompt, asking it to act as an expert archaeologist. The prompt instructs the model to:\n1.  Analyze the provided multimodal image.\n2.  Determine if there is evidence of anthropogenic (human-made) features.\n3.  Provide a confidence score for its assessment.\n4.  Give a clear, concise rationale for its decision.\n5.  Guess the type of feature if possible (e.g., geoglyph, settlement, causeway).\n\nTo optimize costs and speed, we cache the LLM responses. If the same tile is analyzed again, the cached result is returned instead of making a new API call.","metadata":{"papermill":{"duration":0.024696,"end_time":"2025-06-17T15:34:40.542231","exception":false,"start_time":"2025-06-17T15:34:40.517535","status":"completed"},"tags":[]}},{"id":"b2a160a8","cell_type":"code","source":"# === LLM ANALYSIS FUNCTIONS WITH CACHING ===\ndef create_composite_image_for_llm(row):\n    \"\"\"Creates a composite image (RGB, NDVI, Slope) for the LLM.\"\"\"\n    rgb_img = cv2.imread(row['rgb_path'])\n    \n    with rasterio.open(row['ndvi_path']) as src: ndvi_raw = src.read(1)\n    with rasterio.open(row['slope_path']) as src: slope_raw = src.read(1)\n\n    ndvi_color = (plt.cm.viridis(cv2.normalize(ndvi_raw, None, 0, 255, cv2.NORM_MINMAX))[:, :, :3] * 255).astype(np.uint8)\n    slope_color = (plt.cm.magma(cv2.normalize(slope_raw, None, 0, 255, cv2.NORM_MINMAX))[:, :, :3] * 255).astype(np.uint8)\n\n    composite = np.hstack([cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB), ndvi_color, slope_color])\n\n    cv2.putText(composite, 'RGB', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    cv2.putText(composite, 'NDVI', (522, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    cv2.putText(composite, 'Slope', (1034, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    \n    return composite\n\ndef encode_image_to_base64(image_np):\n    \"\"\"Encodes a numpy image array into a base64 string.\"\"\"\n    image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n    _, buffer = cv2.imencode('.jpeg', image_bgr)\n    return base64.b64encode(buffer).decode('utf-8')\n\n\ndef analyze_candidate_with_gpt4o(row, config):\n    \"\"\"\n    Sends a candidate to GPT-4o with retries and caching.\n    \"\"\"\n    cache_file = config['CACHE_DIR'] / f\"{row['tile_id']}.json\"\n    if cache_file.exists():\n        return json.loads(cache_file.read_text())\n\n    max_retries = 3\n    retry_delay = 5\n\n    for attempt in range(max_retries):\n        try:\n            composite_img = create_composite_image_for_llm(row)\n            base64_image = encode_image_to_base64(composite_img)\n\n            expert_prompt = (\n                \"You are an expert remote sensing archaeologist specializing in the Amazon Basin. \"\n                \"Analyze the following composite image which contains three panels: 1. True-color RGB, 2. NDVI (vegetation index), 3. Slope (topography). \"\n                \"Your task is to identify potential anthropogenic features such as geoglyphs, earthworks, or ancient settlements. \"\n                \"Look for unnatural geometric patterns (straight lines, right angles, circles), unusual vegetation patterns, or subtle earthworks. \"\n                \"Respond ONLY with a valid JSON object: {\\\"contains_anthropogenic_features\\\": boolean, \\\"confidence_score\\\": float (0.0-1.0), \"\n                \"\\\"rationale\\\": string, \\\"feature_type_guess\\\": string}.\"\n            )\n\n            client = openai.OpenAI(api_key=OPENAI_API_KEY)\n            response = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[\n                    {\"role\": \"user\",\n                     \"content\": [\n                         {\"type\": \"text\", \"text\": expert_prompt},\n                         {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n                     ]}\n                ],\n                temperature=0.1,\n                max_tokens=300,\n                response_format={\"type\": \"json_object\"}\n            )\n            \n            content = response.choices[0].message.content\n            if content is None:\n                raise ValueError(\"API returned None content\")\n\n            result = json.loads(content)\n            \n            cache_file.write_text(json.dumps(result))\n            return result\n        \n        except (ValueError, json.JSONDecodeError, openai.APIError) as e:\n            print(f\"Attempt {attempt + 1}/{max_retries} failed for tile {row['tile_id']}: {e}. Retrying in {retry_delay}s...\")\n            if attempt < max_retries - 1:\n                time.sleep(retry_delay)\n            else:\n                print(f\"All retries failed for tile {row['tile_id']}.\")\n                return {\"rationale\": f\"API call failed after {max_retries} attempts\", \"confidence_score\": 0.0, \"contains_anthropogenic_features\": False, \"feature_type_guess\": \"error\"}\n\n    return {\"rationale\": \"API call failed after all retries\", \"confidence_score\": 0.0, \"contains_anthropogenic_features\": False, \"feature_type_guess\": \"error\"}","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:56.986327Z","iopub.execute_input":"2025-06-17T16:20:56.987193Z","iopub.status.idle":"2025-06-17T16:20:57.002326Z","shell.execute_reply.started":"2025-06-17T16:20:56.987161Z","shell.execute_reply":"2025-06-17T16:20:57.001253Z"},"papermill":{"duration":0.231048,"end_time":"2025-06-17T15:34:40.796048","exception":false,"start_time":"2025-06-17T15:34:40.565","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"de29d375","cell_type":"code","source":"# === LLM ANALYSIS EXECUTION ===\ndf_final_results = pd.DataFrame()\n\nif not df_candidates.empty:\n    num_to_analyze = min(CONFIG['LLM_ANALYSIS_COUNT'], len(df_candidates))\n    print(f\"--- Sending {num_to_analyze} High-Potential Candidates to GPT-4o ---\")\n    \n    llm_results = []\n    for _, row in tqdm(df_candidates.head(num_to_analyze).iterrows(), total=num_to_analyze, desc=\"LLM Analysis\"):\n        analysis = analyze_candidate_with_gpt4o(row, CONFIG)\n        llm_results.append({**row.to_dict(), **analysis})\n    \n    df_final_results = pd.DataFrame(llm_results)\n    final_path = CONFIG['RESULTS_DIR'] / 'final_llm_analysis.csv'\n    df_final_results.to_csv(final_path, index=False)\n    \n    print(f\"\\nLLM analysis complete. Final results saved to {final_path}\")\n    print(\"\\n--- GPT-4o Analysis Results ---\")\n    display(df_final_results[['tile_id', 'contains_anthropogenic_features', 'confidence_score', 'feature_type_guess', 'rationale']])\nelse:\n    print(\"Skipping LLM analysis as no candidate tiles were generated.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:20:57.00443Z","iopub.execute_input":"2025-06-17T16:20:57.004755Z","iopub.status.idle":"2025-06-17T16:21:15.372921Z","shell.execute_reply.started":"2025-06-17T16:20:57.004731Z","shell.execute_reply":"2025-06-17T16:21:15.372035Z"},"papermill":{"duration":11.421027,"end_time":"2025-06-17T15:34:52.238747","exception":false,"start_time":"2025-06-17T15:34:40.81772","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"ab5658ef","cell_type":"markdown","source":"<a id='stage4'></a>\n## 4. Results Visualization & Interpretation\n\nThe final step is to visualize our findings in a geographical context. A table of results is useful, but an interactive map provides a much more intuitive and powerful way to explore potential archaeological sites.\n\nWe will use `folium` to create an HTML map centered on our Area of Interest. Each tile that the LLM identified as containing potential anthropogenic features will be marked. Clicking on a marker will reveal a popup containing:\n\n1.  The composite image that was shown to the LLM.\n2.  The LLM's confidence score and rationale.\n\nThis provides an immediate, verifiable, and interactive summary of the project's discoveries.","metadata":{"papermill":{"duration":0.02289,"end_time":"2025-06-17T15:34:52.284945","exception":false,"start_time":"2025-06-17T15:34:52.262055","status":"completed"},"tags":[]}},{"id":"776558b2","cell_type":"code","source":"# === INTERACTIVE MAP VISUALIZATION ===\ndef create_results_map(df, config):\n    \"\"\"\n    Generates a Folium map with switchable base layers (Positron and Satellite) \n    and markers for positive LLM results.\n    \"\"\"\n    aoi = config['AOI_BOUNDS']\n    map_center = [(aoi['south'] + aoi['north']) / 2, (aoi['west'] + aoi['east']) / 2]\n    \n    m = folium.Map(location=map_center, zoom_start=9, tiles='CartoDB positron', attr='CartoDB positron')\n\n    folium.TileLayer(\n        'Esri_WorldImagery',\n        attr='Esri | Earthstar Geographics, CNES/Airbus DS, P-VUE, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community'\n    ).add_to(m)\n    \n    positive_results = df[df['contains_anthropogenic_features'] == True]\n    if positive_results.empty:\n        print(\"No positive features identified by the LLM to display on the map.\")\n        return m\n        \n    print(f\"Adding {len(positive_results)} positive sites to the map...\")\n\n    marker_cluster = MarkerCluster().add_to(m)\n\n    for _, row in positive_results.iterrows():\n        composite_img = create_composite_image_for_llm(row)\n        encoded = encode_image_to_base64(cv2.resize(composite_img, (0,0), fx=0.4, fy=0.4))\n        \n        html = f'''\n        <h4>Tile: {row['tile_id']}</h4>\n        <b>LLM Guess:</b> <span style=\"color: #4CAF50; font-weight: bold;\">{row['feature_type_guess'].upper()}</span><br>\n        <b>Confidence:</b> {row['confidence_score']:.2f}<br>\n        <b>Rationale:</b> <em>{row['rationale']}</em><br>\n        <img src=\"data:image/jpeg;base64,{encoded}\" width=\"600px\" alt=\"Composite Image\">\n        '''\n        \n        iframe = folium.IFrame(html, width=640, height=310)\n        popup = folium.Popup(iframe, max_width=640)\n        \n        folium.Marker(\n            location=[row['lat'], row['lon']],\n            popup=popup,\n            tooltip=f\"Candidate: {row['tile_id']} (Click to see details)\",\n            icon=folium.Icon(color='green', icon='search', prefix='fa')\n        ).add_to(marker_cluster)\n        \n    folium.LayerControl().add_to(m)\n        \n    return m\n\nif not df_final_results.empty:\n    results_map = create_results_map(df_final_results, CONFIG)\n    map_path = CONFIG['RESULTS_DIR'] / 'interactive_results_map.html'\n    results_map.save(map_path)\n    print(f\"Interactive map saved to {map_path}\")\n    display(results_map)\nelse:\n    print(\"No final LLM results to create a map from.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-17T16:21:15.373849Z","iopub.execute_input":"2025-06-17T16:21:15.374262Z","iopub.status.idle":"2025-06-17T16:21:15.813253Z","shell.execute_reply.started":"2025-06-17T16:21:15.37423Z","shell.execute_reply":"2025-06-17T16:21:15.812279Z"},"papermill":{"duration":0.061041,"end_time":"2025-06-17T15:34:52.368587","exception":false,"start_time":"2025-06-17T15:34:52.307546","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"190af8bd","cell_type":"markdown","source":"<a id='conclusion'></a>\n## 5. Conclusion & Next Steps\n\nThis notebook demonstrated a powerful, hybrid pipeline for AI-assisted archaeological discovery in the Amazon. By intelligently combining classical computer vision for broad-scale filtering and a state-of-the-art multimodal LLM for deep, expert-level analysis, we have created a system that is both effective and cost-efficient. The interactive map provides a dynamic and intuitive way to explore the model's findings, complete with visual evidence and the AI's reasoning—a crucial tool for subsequent validation by archaeologists.\n\n**Future work could include:**\n- **Expanding Data Sources:** Integrating high-resolution LiDAR or SAR data where available for enhanced topographical and sub-canopy analysis.\n- **Improving CV Filters:** Developing more sophisticated classical algorithms, potentially using self-supervised learning on known sites to detect more subtle anomalies.\n- **Fine-tuning the LLM:** Using a dataset of confirmed sites to fine-tune a model, which could improve its accuracy and reduce verbosity, leading to lower API costs.\n- **Automated Validation:** Cross-referencing flagged locations with historical maps, academic papers, and known site databases to automatically triage the most promising candidates.\n\nWe believe this approach represents a significant step forward in applying AI to solve real-world scientific grand challenges.","metadata":{"papermill":{"duration":0.022465,"end_time":"2025-06-17T15:34:52.414682","exception":false,"start_time":"2025-06-17T15:34:52.392217","status":"completed"},"tags":[]}},{"id":"a63c709c","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022958,"end_time":"2025-06-17T15:34:52.46187","exception":false,"start_time":"2025-06-17T15:34:52.438912","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}
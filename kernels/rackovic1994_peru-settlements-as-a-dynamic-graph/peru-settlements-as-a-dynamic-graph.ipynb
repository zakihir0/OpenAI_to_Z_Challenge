{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101597,"databundleVersionId":12334818,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![https://www.meisterdrucke.uk/kunstwerke/1260px/American%20School%20-%20Caripuna%20Indians%20with%20tapir%20from%20The%20Amazon%20and%20Madeira%20Rivers%20by%20Franz%20Keller%201874%20%20-%20%28MeisterDrucke-80644%29.jpg](http://www.meisterdrucke.uk/kunstwerke/1260px/American%20School%20-%20Caripuna%20Indians%20with%20tapir%20from%20The%20Amazon%20and%20Madeira%20Rivers%20by%20Franz%20Keller%201874%20%20-%20%28MeisterDrucke-80644%29.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Settlements Progression\n\nHuman movements and settlement locations are not random. In order to predict potential historical sites, one could look at the progression of settlements over time. Particualty, I want to build a graph with geo-locations of known sites, and make it dynamic, showing the new settlements, and removing the abandoned ones, with connections between proximal pairs.\n\nFor this, I will start by scraping wikipedia links within relevant categories. ","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport time\nimport csv\nimport time\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom geopy.distance import great_circle\nimport networkx as nx\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nfrom IPython.display import Image\n\n# Base URL for Wikipedia\nBASE_URL = \"https://en.wikipedia.org\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:25:06.608576Z","iopub.execute_input":"2025-06-10T08:25:06.609861Z","iopub.status.idle":"2025-06-10T08:25:06.616672Z","shell.execute_reply.started":"2025-06-10T08:25:06.609826Z","shell.execute_reply":"2025-06-10T08:25:06.615764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Scraping Archeological Sites of Peru\n\nLet's start with Peru region, and scrape the wikipedia links available in this category:\n\n[https://en.wikipedia.org/wiki/List_of_archaeological_sites_in_Peru](https://en.wikipedia.org/wiki/List_of_archaeological_sites_in_Peru)","metadata":{}},{"cell_type":"code","source":"def get_site_links(category_url:str) -> list:\n    \"\"\"\n    Extracts links from the provided wikipedia category.\n\n    -------\n    :param category_url: url pointing to the wikipedia list of relevant sites\n\n    -------\n    :returns: List of urls corresponding to the liked wikipedia pages within the category\n    \"\"\"\n    response = requests.get(category_url)\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    site_links = []\n\n    content_div = soup.find(\"div\", {\"class\": \"mw-parser-output\"})\n    if content_div:\n        for ul in content_div.find_all(\"ul\"):\n            for li in ul.find_all(\"li\"):\n                a_tag = li.find(\"a\")\n                if a_tag and a_tag.has_attr(\"href\") and not a_tag['href'].startswith(\"#\"):\n                    href = a_tag['href']\n                    if href.startswith('http'):\n                        full_url = href\n                    else:\n                        full_url = BASE_URL + a_tag['href']\n                    if 'https://en.wikipedia.org/wiki/' in full_url:\n                        site_links.append(full_url)\n    return site_links","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:25:06.629693Z","iopub.execute_input":"2025-06-10T08:25:06.630317Z","iopub.status.idle":"2025-06-10T08:25:06.637219Z","shell.execute_reply.started":"2025-06-10T08:25:06.630292Z","shell.execute_reply":"2025-06-10T08:25:06.636322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Category page for Archaeological sites in Peru\nCATEGORY_URL = f\"{BASE_URL}/wiki/List_of_archaeological_sites_in_Peru\"\nsite_links = get_site_links(CATEGORY_URL)\nprint(f'Found {len(site_links)} potential links.')\nfor i in range(10):\n    print(site_links[np.random.randint(len(site_links))])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:25:06.663092Z","iopub.execute_input":"2025-06-10T08:25:06.663584Z","iopub.status.idle":"2025-06-10T08:25:06.876242Z","shell.execute_reply.started":"2025-06-10T08:25:06.663555Z","shell.execute_reply":"2025-06-10T08:25:06.875318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For each of these links, we will check if there is a geo-location, i.e., latitude and longitude.\nIf available, we scrape the data from there.\n\nBesides coordinates, we want to have the foundation and abandonment dates. Often, these info are in the info-box of the wikipedia page. If it is not there, we extract introductory text, and look for potential info there. Wherever there are indicators like 'AD', 'BC', 'years ago', we look if there is a numeric 'word' next to it, and take that as a potential date. Further, if next to it we have words like 'founded' or their synonyms, we take that as an answer. If no clear indicator is found, but there are some potential dates in the text, we simply order them, and take the earliest one for the foundation date, and the latest one (if the list has at least two distinct elements), as the abandonment date.\n\nFinally, if this still didn't yield an answer, we look if the info-box contains a culture field. Then the age of the site is assumed to be the same as the spread of that culture. ","metadata":{}},{"cell_type":"code","source":"def extract_site_data(url:str) -> dict:\n    '''\n    Extracts the data about the archeological site. \n    If it can be located precieslly, with geo coordinates.\n    Extracts location, and foundataion and abandonment dates. If on of hte later is not available, also extracts intro text and culture info, that can be used to infer the dates. \n        \n    -------\n    :param url: url pointing to the wikipedia page\n\n    -------\n    :returns: Dictionary containing the data about the settlement, extracted from the given url. \n    '''\n\n    PERIOD_INDICATORS = ['years ago', 'years old']\n    period_indicators_0 = ['B.P.','A.D.','C.E.','B.C.', 'B.C.E.']\n    for element in period_indicators_0:\n        PERIOD_INDICATORS.append(element)\n        PERIOD_INDICATORS.append(element.replace(\".\",\". \").strip())\n        PERIOD_INDICATORS.append(''.join(element.split('.')))\n\n    # Synonyms for 'founded' and 'abandoned'\n    FOUNDED_SYNONYMS = ['founded', 'built', 'establish', 'set up', 'create', 'construct']\n    ABANDON_SYNONYMS = ['abandon', 'destroy', 'deserted']\n\n    def extract_lat_lon(soup:object) -> tuple[float,float]:\n        \"\"\"\n        Extract geo coordinates of the scraped wiki link, if they exist\n\n        -------\n        :param soup: BeautifulSoup object, html representation of the wikipedia page\n\n        -------\n        :returns: latitude and longitude in decimal format, if available in the wiki page header.\n        \"\"\"\n\n        def convert_coordinate(coordinate:str)->str:\n            \"\"\"\n            Takes in a coordiante, such as a latitude or longitude, assuming it has degrees and optionally minutes, seconds '15°49′S', and makes sure it is writtten properly\n            \n            -------\n            :param coordinate: Geo-coordinate, latitude or longitude, in a form of degrees/minutes/seconds, like '13°37′05″S', with possibly missing one of the three\n\n            -------\n            :returns: corrected geo-coordinate, i.e., the same value but made sure to obey the strict rule of two digits in each of teh three units\n            \"\"\"\n            latitude = ''\n            if '°' in coordinate:\n                degrees = coordinate.split('°')\n                deg = re.sub('[^0-9]',' ', degrees[0]).strip()[:2]\n                latitude += deg + '°'\n                rest = degrees[1]\n                if '′' in rest:\n                    minutes = rest.split('′')\n                    mins = minutes[0].split('.')[0].strip()\n                    if len(mins) == 1:\n                        mins = '0'+mins\n                    latitude += mins + '′'\n                    rest = minutes[1]\n                    if '″' in rest:\n                        seconds = rest.split('″')\n                        sec = seconds[0].split('.')[0].strip()\n                        if len(sec) == 1:\n                            sec = '0'+sec\n                        latitude += sec + '″'\n                    else:\n                        latitude += '00″'\n                else:\n                    latitude += '00′00″'\n            latitude += coordinate[-1]\n            return latitude\n\n        def dms_to_decimal(coordinate:str) -> float:\n            \"\"\"\n            Convert a DMS (degrees, minutes, seconds) coordinate string to decimal degrees.\n            Example input: '13°37′05″S'\n\n            -------\n            :param coordinate: Geo-coordinate, latitude or longitude, in a form of degrees/minutes/seconds\n\n            -------\n            :returns: decimal form of the inserted coordinate\n            \"\"\"\n            # Regex to extract degrees, minutes, seconds, and direction\n            pattern = r\"(\\d+)°(\\d+)′(\\d+)″([NSEW])\"\n            match = re.match(pattern, coordinate.strip())\n            if not match:\n                raise ValueError(f\"Invalid coordinate format: {coordinate}\")\n            degrees, minutes, seconds, direction = match.groups()\n            decimal = int(degrees) + int(minutes) / 60 + int(seconds) / 3600\n            if direction in ['S', 'W']:\n                decimal *= -1\n            return decimal\n\n        # Might be this format, with separate classes for lat and lon\n        lat = soup.find('span', {'class':'latitude'})\n        lon = soup.find('span', {'class':'longitude'})\n        if lat and lon:\n            lat = lat.text.strip()\n            lat = convert_coordinate(lat)\n            lon = lon.text.strip()\n            lon = convert_coordinate(lon)\n            return dms_to_decimal(lat), dms_to_decimal(lon)\n\n        # Or this format, for geo decimal coordinates\n        geo_dec = soup.find('span', class_='geo-dec')\n        if geo_dec:\n            coords = geo_dec.get_text(strip=True).split(',')\n            if len(coords) == 2:\n                lat, lon = coords[0].strip(), coords[1].strip()\n                return lat, lon\n        # if both fail, return None\n        return None\n\n    def extract_age(soup:object)->tuple[bool,bool,bool,int,int,str]:\n        \"\"\"\n        Extract the age / fundation period of the scraped wiki link, if available\n\n        -------\n        :param soup: BeautifulSoup object, html representation of the wikipedia page\n\n        -------\n        :returns: age_founded_flag, age_abandoned_flag, culture_flag - booleans indicating if the info about foundation date, abandonment date, or culture, repsectivelly, is extracted. age_founded - settlement foundation date, age_abandoned - settlement abandonmebt date, culture - culture that inhabited the settlement\n        \"\"\"\n\n        age_founded = None\n        age_founded_flag = False\n        age_abandoned = None\n        age_abandoned_flag = False\n        culture = None\n        culture_flag = False\n\n        # First case, if it is in the info box\n        infobox = soup.find('table', class_='infobox vcard')\n        if infobox:\n            rows = infobox.find_all('tr')\n            for row in rows:\n                header = row.find('th', class_=\"infobox-label\")\n                # If there is a header sayin 'Founded', extract the date from there\n                if header and 'Founded' in header.get_text(strip=True):\n                    boxdata = row.find('td', class_='infobox-data')\n                    if boxdata:\n                        age_founded = boxdata.get_text(strip=True)\n                        age_founded = re.sub(\"[\\[].*?[\\]]\", \"\", age_founded)\n                        if any(char.isdigit() for char in age_founded):\n                            age_founded_flag = True\n\n                if header and 'Abandoned' in header.get_text(strip=True):\n                    boxdata = row.find('td', class_='infobox-data')\n                    if boxdata:\n                        age_abandoned = boxdata.get_text(strip=True)\n                        age_abandoned = re.sub(\"[\\[].*?[\\]]\", \"\", age_abandoned)\n                        if any(char.isdigit() for char in age_abandoned):\n                            age_abandoned_flag = True\n\n                # Check if there is a cluture it belongs to\n                if header and 'Culture' in header.get_text(strip=True):\n                    boxdata = row.find('td', class_='infobox-data')\n                    if boxdata:\n                        culture = boxdata.get_text(strip=True)\n                        culture = re.sub(\"[\\[].*?[\\]]\", \"\", culture).lower()\n                        culture_flag = True\n\n        return age_founded_flag, age_abandoned_flag, culture_flag, age_founded, age_abandoned, culture\n\n\n    def transforimg_textual_years(textual_years:list)->list:\n        \"\"\"\n        Takes a list of strings containing period indicators (years), and transforms them into numerical values.\n        Transformed years are integers, where years BC are negative, and AD are positive intigers.\n\n        -------\n        :param textual_years: List of strings of the form \"1200 BC\" or similar\n\n        -------\n        :returns: list of integers corresponding to the string values from 'textual_years'. E.g.,: transforimg_textual_years(['1200 BC']) -> [-1200] \n        \"\"\"\n        remove_char = ['.',',']\n        transformed_ages = []\n\n        for ty in textual_years:\n            indicators_flag = False\n            for i, pe in enumerate(PERIOD_INDICATORS):\n                if pe in ty:\n                    indicators_flag = True\n                    for rc in remove_char:\n                        ty = ty.replace(rc, \"\")\n                    ty = re.sub('[^0-9]',' ', ty).strip().split(' ')\n                    ty = [y for y in ty if len(y)>0]\n                    if i < 5:\n                        transformed_ages += [2000-int(y) for y in ty]\n                    elif i < 11:\n                        transformed_ages += [-int(y) for y in ty]\n                    else:\n                        transformed_ages += [int(y) for y in ty]\n                    break\n            if not indicators_flag:\n                for rc in remove_char:\n                    ty = ty.replace(rc, \"\")\n                ty = re.sub('[^0-9]',' ', ty).strip().split(' ')\n                ty = [y for y in ty if len(y)>0]\n                transformed_ages += [int(y) for y in ty]\n\n        return transformed_ages\n\n    def extract_intro(soup:object) -> tuple[str, list, bool, bool, str, str]:\n        \"\"\"\n        Extract the inttro section from the wikipedia page.\n        Removes the citations, and isolates parts of text with dates, that might indicate the age of founding or deserting the settlement.\n\n        -------\n        :param soup: BeautifulSoup object, html representation of the wikipedia page\n\n        -------\n        :returns: lead_text - intro text of the wikipedia page. encountered_ages - list of ages with their era indicators, from the intro text. age_founded_flag/age_abandoned_flag - flag indicating if the foundation/abandonment date is found in the text. age_founded/age_abandoned - textual year of the foundation/abandonment, if found.\n    \n        \"\"\"\n        # Text content of the leading paragraphs (intro of the wiki page)\n        content_body = soup.find('div', {'id': 'mw-content-text'})\n        content_div = content_body.find('div', {'class': 'mw-parser-output'})\n\n        lead_text = []\n        for p in content_div.find_all('p', recursive=True):\n            text = p.get_text(strip=False)\n            if text:\n                lead_text.append(text)\n            # Stop collecting once we reach a <p> after which comes a heading or non-intro content\n            next_sibling = p.find_next_sibling()\n            if next_sibling and next_sibling.name == 'div':\n                break\n        lead_text = ''.join(lead_text)\n        lead_text = re.sub(\"[\\[].*?[\\]]\", \"\", lead_text)\n\n        age_founded = None\n        age_founded_flag = False\n        age_abandoned = None\n        age_abandoned_flag = False\n\n        # Observe parts of text that contain age info\n        encountered_ages = []\n        for i, pe in enumerate(PERIOD_INDICATORS):\n            idx = lead_text.find(pe)\n            if idx > 0:\n                idx_25 = max(0,idx-25)\n                text_part = lead_text[idx_25:idx].lower() # check a short text part prior to indicator\n                text_year = text_part.split()[-1] # take only the very last word\n                if any(char.isdigit() for char in text_year):\n                    encountered_ages.append(text_year+' '+pe)\n                    for fs in FOUNDED_SYNONYMS:\n                        if fs in text_part:\n                            age_founded = text_year+' '+pe\n                            age_founded_flag = True\n                    for abs in ABANDON_SYNONYMS:\n                        if abs in text_part:\n                            age_abandoned = text_year+' '+pe\n                            age_abandoned_flag = True\n                else: # If no digits prior to age indicators, check the word after\n                    idx_25 = min(len(lead_text),idx+len(pe)+25)\n                    text_part_posterior = lead_text[idx+len(pe):idx_25].lower()\n                    text_year = text_part_posterior.split()[0]\n                    if any(char.isdigit() for char in text_year):\n                        encountered_ages.append(text_year+' '+pe)\n                        for fs in FOUNDED_SYNONYMS:\n                            if fs in text_part:\n                                age_founded = text_year+' '+pe\n                                age_founded_flag = True\n                        for abs in ABANDON_SYNONYMS:\n                            if abs in text_part:\n                                age_abandoned = text_year+' '+pe\n                                age_abandoned_flag = True\n\n\n        return lead_text, encountered_ages, age_founded_flag, age_abandoned_flag, age_founded, age_abandoned\n\n\n\n    data = {}\n    time.sleep(1)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    try:\n        title = soup.find('span', {'class':'mw-page-title-main'}).text\n    except:\n        return None\n    # Name of the site as a wiki page title - it hte page doesn't ahve a title, we don't want it\n    data['NAME'] = title\n    lat_lon = extract_lat_lon(soup)\n    if lat_lon:\n        data['LATITUDE'] = lat_lon[0]\n        data['LONGITUDE'] = lat_lon[1]\n    else: # If we don't have the loaction, we don't want this data at all\n        return None\n\n    print(\"Scraping: \", url)\n\n    age_founded_flag, age_abandoned_flag, culture_flag, age_founded, age_abandoned, culture = extract_age(soup)\n    if age_founded_flag:\n        data['FOUNDED'] = age_founded\n    if age_abandoned_flag:\n        data['ABANDONED'] = age_abandoned\n    if culture_flag:\n        data['CULTURE'] = culture\n\n    # If we have both the construction and abandonment dates, that's it.\n    # Otherwise, we want to extract the intro text and try to fidn the age indicators there.\n    if not (age_founded_flag and age_abandoned_flag):\n        lead_text, encountered_ages, age_founded_flag, age_abandoned_flag, age_founded, age_abandoned = extract_intro(soup)\n        if age_founded_flag and 'FOUNDED' not in data.keys():\n            data['FOUNDED'] = age_founded\n        if age_abandoned_flag and 'ABANDONED' not in data.keys():\n            data['ABANDONED'] = age_abandoned\n\n    # If any of the extracted ages from the intro file was next to the words like 'Foudned' or 'abandoned', we take that as a definite data.\n    # Otherwise, we take a list of all extracted years, and the earliest one is assumed to be the foundation date, while the most recent one (if different) is the abandonmenet year.\n    if 'FOUNDED' in data.keys():\n        numeric_years = transforimg_textual_years([data['FOUNDED']])\n        data['FOUNDED NUMERIC'] = min(numeric_years)\n    elif len(encountered_ages):\n        numeric_years = transforimg_textual_years(encountered_ages)\n        data['FOUNDED NUMERIC'] = min(numeric_years)\n    if 'ABANDONED' in data.keys():\n        numeric_years = transforimg_textual_years([data['ABANDONED']])\n        data['ABANDONED NUMERIC'] = max(numeric_years)\n    elif len(encountered_ages):\n        numeric_years = transforimg_textual_years(encountered_ages)\n        if len(numeric_years)>1:\n            data['ABANDONED NUMERIC'] = max(numeric_years)\n\n    # It's (almost) safe to assum that if the given year is greater than 2000, it was supposed to be 2000BC, not AD\n    if 'FOUNDED NUMERIC' in data.keys(): \n        if data['FOUNDED NUMERIC'] >= 2000:\n            data['FOUNDED NUMERIC'] *= -1\n    if 'ABANDONED NUMERIC' in data.keys():\n        if data['ABANDONED NUMERIC'] >= 2000:\n            data['ABANDONED NUMERIC'] *= -1\n            \n    # If we still don't have these, we keep the text\n    if 'FOUNDED NUMERIC' not in data.keys() or 'ABANDONED NUMERIC' not in data.keys():\n        data['TEXT'] = lead_text\n\n    return data\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:25:06.877987Z","iopub.execute_input":"2025-06-10T08:25:06.878355Z","iopub.status.idle":"2025-06-10T08:25:06.914121Z","shell.execute_reply.started":"2025-06-10T08:25:06.878334Z","shell.execute_reply":"2025-06-10T08:25:06.913081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## NOTE\n\nThe above function is a very good place to use GPT (if you can afford it). Within the function \"extract_intro\" I look for potential foundation and abandonment years, however, even if I don't find clear indicators of one of them, I extract whatever years I could find in this short text. Instead, one should use this piece of text as a prompt context, and ask the LLM something like:\n\n\"\"\"\nThis is a wikipedia headline for the archeological site of {title}, in Peru:\n{extracted_intro_text}\n\nGive me the foundation date of the site, and, if available, the year when it was abandoned.\nUse negative integers for years BC, and positive for AD. Return only numbers, no alphabetical characters!\nReturn the data in the list format, like this:\n[-2500, 1430]\n\"\"\"\n\nFurther, if the intro still didn't provide years, (since i was avoiding to use GPT) down in this notebook I use other possible tricks to  infer the dates. Like, looking for the connection between the cultures and a settlement or so. However, GPT should do a better job with this, even without context intro. You can just provide a prompt similar to the above one without context, indicating only the site name.","metadata":{}},{"cell_type":"code","source":"all_data = []\ncultures = {}\nfor url in site_links:\n    data = extract_site_data(url)\n    if data:\n        all_data.append(data)\n        if 'CULTURE' in data.keys():\n            culture = data['CULTURE']\n            if culture not in cultures.keys():\n                cultures[culture] = 1\n            else:\n                cultures[culture] += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:25:06.914908Z","iopub.execute_input":"2025-06-10T08:25:06.915189Z","iopub.status.idle":"2025-06-10T08:31:43.877543Z","shell.execute_reply.started":"2025-06-10T08:25:06.91517Z","shell.execute_reply":"2025-06-10T08:31:43.8767Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This gave us a list of extracted data. Those where the dates are not found/estimates still keep the wikipedia intro text. It can be potentially used as a prompt context to ask GPT if it can find or guess the age. (Sometimes the info is stated there clearly, but it is not numeric, so our function couldn't extract it. For example \"The settlement was build in the first century....\")\n\nAdditionally, we built a dictionary counting appearances of cultures encountered in the pages, as well as their respective number of occurrences. ","metadata":{}},{"cell_type":"code","source":"print(f\"We extracted the total of {len(all_data)} sites.\")\nprint(\"The cultures encountered within info-boxes: \")\nprint(cultures)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:43.879709Z","iopub.execute_input":"2025-06-10T08:31:43.879983Z","iopub.status.idle":"2025-06-10T08:31:43.884968Z","shell.execute_reply.started":"2025-06-10T08:31:43.879959Z","shell.execute_reply":"2025-06-10T08:31:43.884169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most of the encountered values in these pages correspond to Inca culture.\nI made another dictionary, extracting the data from this wikipedia page\n[https://en.wikipedia.org/wiki/Pre-Columbian_Peru](https://en.wikipedia.org/wiki/Pre-Columbian_Peru)\nthat lists cultures and their respective realm periods. Note that years before CE are counted as negative integers.","metadata":{}},{"cell_type":"code","source":"# https://en.wikipedia.org/wiki/Pre-Columbian_Peru\nCulturesPeru = {\n    \"paiján\" :      [-11000, -8000],\n    \"lauricocha\" :  [-10000, -2500],\n    \"casma–sechin\": [-3500, 200],\n    \"norte chico\" : [-3500, -1800],\n    \"caral-supe\" :  [-3500, -1800],\n    \"cupisnique\" :  [-1500, -500],\n    \"chavín\" :      [-900, -250],\n    \"paracas\" :     [-800, -100],\n    \"nazca\" :       [-100, 800],\n    \"moche\" :       [100, 800],\n    \"wari\" :        [500, 1000],\n    \"tiwanaku\" :    [600, 1000],\n    \"chachapoya\" :  [800, 1470],\n    \"chimú\" :       [900, 1470],\n    \"chimor\" :      [900, 1470],\n    \"ichma\" :       [1100, 1469],\n    \"chanka\" :      [1200, 1438],\n    \"inca\" :        [1200, 1572]\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:43.885809Z","iopub.execute_input":"2025-06-10T08:31:43.886088Z","iopub.status.idle":"2025-06-10T08:31:43.904781Z","shell.execute_reply.started":"2025-06-10T08:31:43.886064Z","shell.execute_reply":"2025-06-10T08:31:43.903989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_cultures_timelines(CulturesDict:dict, title:str=None):\n    \"\"\"\n    Plots temporal spread of each per-Colombian culture.\n    \n    -------\n    :param CulturesDict: Dictionary with culture names as keys and touple of their starting and ending year, as a value.\n    :param title: Figure title\n    \"\"\"\n    left_limit, right_limit = 0, 0\n    fig, ax = plt.subplots(figsize=(15,4))\n    for i, key in enumerate(CulturesDict.keys()):\n        begin, end = CulturesDict[key][0], CulturesDict[key][1]\n        if begin < left_limit:\n            left_limit = 0.+begin\n        if end > right_limit:\n            right_limit = 0.+end\n        duration = end-begin\n        plt.barh(i, width = duration, left = begin, height = 0.9)\n        plt.text(begin + duration + 50, i, key.capitalize(), va='center')  # adjust `+10` as needed for spacing\n    ax.spines[['right', 'top', 'left']].set_visible(False)\n    plt.yticks([])\n    plt.xlim(left_limit-100, right_limit+300)\n    plt.xlabel('Years')\n    if title:\n        plt.title(title)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:43.905608Z","iopub.execute_input":"2025-06-10T08:31:43.905883Z","iopub.status.idle":"2025-06-10T08:31:43.928205Z","shell.execute_reply.started":"2025-06-10T08:31:43.90586Z","shell.execute_reply":"2025-06-10T08:31:43.927252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next figure shows the cultures from this region, and their respective realm periods. ","metadata":{}},{"cell_type":"code","source":"plot_cultures_timelines(CulturesPeru, title='Pre-Columbian Cultures in Peru')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:43.92913Z","iopub.execute_input":"2025-06-10T08:31:43.929371Z","iopub.status.idle":"2025-06-10T08:31:44.12643Z","shell.execute_reply.started":"2025-06-10T08:31:43.929353Z","shell.execute_reply":"2025-06-10T08:31:44.125504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encountered_cultures = {}\nfor key in CulturesPeru.keys():\n    encountered_cultures[key] = 0\nfor key in cultures.keys():\n    for key1 in encountered_cultures.keys():\n        if key1 in key:\n            encountered_cultures[key1] += cultures[key]\n            break\n            \nfig, ax = plt.subplots(figsize=(10,3))\ni = 0\nfor key in encountered_cultures:\n    if encountered_cultures[key] > 0:\n        i += 1\n        plt.barh(i, width = encountered_cultures[key])\n        plt.text(encountered_cultures[key]+0.2, i, key.capitalize(), va='center')  # adjust `+10` as needed for spacing\nax.spines[['right', 'top', 'left']].set_visible(False)\nplt.yticks([])\nplt.xlabel('Count')\nplt.title(\"Occurances per culture\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:44.127514Z","iopub.execute_input":"2025-06-10T08:31:44.127838Z","iopub.status.idle":"2025-06-10T08:31:44.283863Z","shell.execute_reply.started":"2025-06-10T08:31:44.127809Z","shell.execute_reply":"2025-06-10T08:31:44.282894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now using this info, I visit extracted data, and for those that have specified culture, but not dates. I will use these years as estimates of foundation and abandonment. ","metadata":{}},{"cell_type":"code","source":"def culture_based_foundation(data:dict, CulturesDict:dict) ->tuple[dict, bool]:\n    \"\"\"\n    Based on the culture the settlement beloged to, assigns correpsonding foundation and abandonment dates.\n    \n    -------\n    :param data: Dictionary containing main data about the settlment\n    :param CulturesDict: Dictionary with culture names as keys and touple of their starting and ending year, as a value.\n\n    -------\n    :returns: updated data dictionary, and a flag indicating if any edits were made\n    \"\"\"\n    if 'CULTURE' in data.keys():\n        for culture in CulturesDict.keys():\n            if culture in data['CULTURE']:\n                if 'FOUNDED NUMERIC' not in data.keys():\n                    data['FOUNDED NUMERIC'] = CulturesDict[culture][0]\n                if 'ABANDONED NUMERIC' not in data.keys():\n                    data['ABANDONED NUMERIC'] = CulturesDict[culture][1]\n                if 'TEXT' in data.keys():\n                    data.pop('TEXT')\n            return data, True\n    return data, False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:31:29.857765Z","iopub.execute_input":"2025-06-18T12:31:29.858983Z","iopub.status.idle":"2025-06-18T12:31:29.869125Z","shell.execute_reply.started":"2025-06-18T12:31:29.858942Z","shell.execute_reply":"2025-06-18T12:31:29.868236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"complete_years, edited_years, missing_years = 0, 0, 0\nfor i, data in enumerate(all_data):\n    if 'FOUNDED NUMERIC' in data and 'ABANDONED NUMERIC' in data:\n        complete_years += 1\n    else:\n        data, flag = culture_based_foundation(data, CulturesPeru)\n        if flag:\n            complete_years += 1\n            edited_years += 1\n        else:\n            missing_years += 1\n    all_data[i] = data\nprint(f\"Number of sites with complete start and end dates: {complete_years}\")\nprint(f\"Number of sites with start and end dates edited based on their culture: {edited_years}\")\nprint(f\"Number of sites with missing start or end dates: {missing_years}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:44.294701Z","iopub.execute_input":"2025-06-10T08:31:44.295037Z","iopub.status.idle":"2025-06-10T08:31:44.314417Z","shell.execute_reply.started":"2025-06-10T08:31:44.295012Z","shell.execute_reply":"2025-06-10T08:31:44.313159Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Graphs!\n\nIt is finally time to show this data on a map. Below we see the distribution of the encountered sites in space, and also in time.\n\nMost of the foundation years are after 2000 BC.","metadata":{}},{"cell_type":"code","source":"# Set up a basic map\nfig = plt.figure(figsize=(10, 10))\nax = plt.axes(projection=ccrs.PlateCarree())\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.OCEAN)\nax.add_feature(cfeature.LAKES)\nax.add_feature(cfeature.RIVERS)\n\n# Plot points\nall_foundation_dates = []\nfor data in all_data:\n    lat, lon = data['LATITUDE'], data['LONGITUDE']\n    ax.plot(lon, lat, marker='x', color='brown', transform=ccrs.Geodetic())\n    if 'FOUNDED NUMERIC' in data.keys():\n        all_foundation_dates.append(data['FOUNDED NUMERIC'])\n\nplt.title('Archeological Sites in Peru')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:44.315458Z","iopub.execute_input":"2025-06-10T08:31:44.315795Z","iopub.status.idle":"2025-06-10T08:31:45.002079Z","shell.execute_reply.started":"2025-06-10T08:31:44.315764Z","shell.execute_reply":"2025-06-10T08:31:45.00131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,4))\nax.spines[['right', 'top']].set_visible(False)\nplt.hist(all_foundation_dates,bins=50)\nplt.title(\"Distribution of Foundation Dates\")\nplt.ylabel('Count')\nplt.xlabel('Years')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:45.003024Z","iopub.execute_input":"2025-06-10T08:31:45.003289Z","iopub.status.idle":"2025-06-10T08:31:45.475987Z","shell.execute_reply.started":"2025-06-10T08:31:45.003269Z","shell.execute_reply":"2025-06-10T08:31:45.474965Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create Actual Dynamic Graphs\n\nNow with all this nice data, we want to build an adjacency matrix, connecting the nearby sites.\nAdditionally, we observe points in time and select subsets of all the nodes that were 'active' at the time. Since most of the time prior 2000 BC we don't have many changes, the timestamps in that period will be sparser, and posterior ones will be danser (and we will use 1500 AD as the end date, since that is already post-Columbian period).","metadata":{}},{"cell_type":"code","source":"def build_adjecency_matrix(all_data:list, all_foundation_dates:list, timestamps:list, distance_threshold:int, plot_flag:bool=True):\n    \"\"\"\n    Computes adjecency matrix based on world-distances between sites. \n    Further, creates a matrix of age based occurences of sites, so taht at any given timestamp we can which were the cities that were active.\n\n    \n    -------\n    :param all_data: List of dictionaries containing main data about the settlment\n    :param all_foundation_dates: List of foundation years (numeric) for the correpsonding settlemtns\n    :param timestamps: List of timestamps/years for which we check eistance for the correpsonding settlemtns\n    :param distance_threshold: Threshold value for the straight-line distance between two setlmenets (in kilometers) to consider them connected \n    :param plot_flag: If set to True, plots the extracted things\n\n    -------\n    :returns: ajdacency matrix for hte settlements graph, matrix of temporal occurances for each settlemnt and their geo locations (latitude and longitude)    \n    \"\"\"\n    n= len(all_data)\n    adjecency_matrix = np.zeros((n,n))\n    locations_matrix = np.zeros((n,2))\n    times_matrix = np.zeros((n,2)).astype(int)\n    temporal_occurance_matrix = np.zeros((n, len(timestamps)))\n    for i, data in enumerate(all_data):\n        locations_matrix[i] += (data['LATITUDE'], data['LONGITUDE'])\n        if 'FOUNDED NUMERIC' in data.keys():\n            times_matrix[i,0] = data['FOUNDED NUMERIC']\n        else:\n            times_matrix[i,0] = min(all_foundation_dates)\n        if 'ABANDONED NUMERIC' in data.keys():\n            times_matrix[i,1] = data['ABANDONED NUMERIC']\n        else:\n            times_matrix[i,1] = 1500\n        if times_matrix[i,1] < times_matrix[i,0]:\n            times_matrix[i,0] = times_matrix[i,1]\n            times_matrix[i,1] = 1500\n    for i in range(n):\n        for j in range(i,n):\n            adjecency_matrix[i, j] = great_circle(locations_matrix[i], locations_matrix[j]).km\n            adjecency_matrix[j, i] += adjecency_matrix[i,j]\n        temporal_occurance = (times_matrix[i,0] <= np.array(timestamps))*(times_matrix[i,1] >= np.array(timestamps))*1\n        temporal_occurance_matrix[i] += temporal_occurance\n\n    if plot_flag:\n        distance_matrix = adjecency_matrix.copy()\n        distance_matrix[distance_matrix==0] = adjecency_matrix.max()\n        distance_vector = distance_matrix.min(1)\n        fig, ax = plt.subplots(figsize=(12,4))\n        ax.spines[['right', 'top']].set_visible(False)\n        plt.hist(distance_vector,bins=230,color='g')\n        plt.title(\"Histogram of pair-wise site distances\")\n        plt.ylabel(\"count\")\n        plt.xlabel(\"Distances in km\")\n        plt.scatter(distance_threshold,0,color='yellow',marker='d',s=100)\n        plt.show()\n    \n    adjecency_matrix[adjecency_matrix <= distance_threshold]=1\n    adjecency_matrix[adjecency_matrix > distance_threshold]=0\n    adjecency_matrix -= np.eye(n)\n    if plot_flag:\n        fig, ax = plt.subplots(figsize=(7,7))\n        order = np.argsort(np.mean(adjecency_matrix,1))\n        adjecency_matrix_sorted = adjecency_matrix[order]\n        adjecency_matrix_sorted = adjecency_matrix_sorted[:,order]\n        plt.imshow(adjecency_matrix_sorted, cmap='binary')\n        ax.spines[['right', 'top','left','bottom']].set_visible(False)\n        plt.title(\"Adjecency matrix\")\n        plt.xticks([])\n        plt.yticks([])\n        plt.show()\n    \n        fig, ax = plt.subplots(figsize=(7,7))\n        plt.imshow(temporal_occurance_matrix[order], cmap='binary')\n        plt.title(\"Temporal occurances matrix\")\n        ax.spines[['right', 'top','left','bottom']].set_visible(False)\n        plt.xticks([])\n        plt.yticks([])\n        plt.xlabel('Time')\n        plt.ylabel('Sites')\n        plt.show()\n\n    return adjecency_matrix, temporal_occurance_matrix.astype(int), locations_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:45.476985Z","iopub.execute_input":"2025-06-10T08:31:45.477286Z","iopub.status.idle":"2025-06-10T08:31:45.49064Z","shell.execute_reply.started":"2025-06-10T08:31:45.477262Z","shell.execute_reply":"2025-06-10T08:31:45.489673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below we can see informative plots about our nodes/settlements. The barplot shows the distances between the nearest neighbors/settlements, in kilometers (straight line). Yellow triangle shows a selected distance threshold -- the nodes at a distance of 100km or less are considered to be directly connected, to avoid having isolated nodes.\n \nUnder that, we have an adjacency matrix, inferred from this distance threshold (and sorted accordingly). Black pixels indicate that there is a connection between the nodes of the graph.\nFInally, the last plot is a temporal occurrence matrix. There, each row corresponds to a single settlement, and each column to a year from the timestamps list (we defined it in the next cell). Thite pixels there indicate that the observed settlement didn't exist (or wasn't inhabited) at that time.","metadata":{}},{"cell_type":"code","source":"distance_threshold = 100\ntimestamps = np.linspace(min(all_foundation_dates), -3500, 10).tolist() + np.linspace(-3250, -1500, 10).tolist()+ np.linspace(-1400, 1500, 30).tolist()\ntimestamps = [int(t) for t in timestamps]\nprint(f\"Time stamps: {timestamps}\\n\")\nadjecency_matrix, temporal_occurance_matrix, locations_matrix = build_adjecency_matrix(all_data, all_foundation_dates, timestamps, distance_threshold)\nnp.save('Peru_adjecency_matrix.npy',adjecency_matrix)\nnp.save('Peru_temporal_occurance_matrix.npy',temporal_occurance_matrix)\nnp.save('Peru_locations_matrix.npy',locations_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:45.49143Z","iopub.execute_input":"2025-06-10T08:31:45.491655Z","iopub.status.idle":"2025-06-10T08:31:46.749055Z","shell.execute_reply.started":"2025-06-10T08:31:45.491637Z","shell.execute_reply":"2025-06-10T08:31:46.74817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's look at the progression of the settlements over time.\n\nBelow GIF shows settlements by year, and edges shows which settlement pairs are considered neighbors. ","metadata":{}},{"cell_type":"code","source":"graph =nx.from_numpy_array(adjecency_matrix)\n\nfig = plt.figure(figsize=(10, 10))\nax = plt.axes(projection=ccrs.PlateCarree())\n\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.RIVERS)\nax.add_feature(cfeature.OCEAN)\npos = {i: (locations_matrix[i][1], locations_matrix[i][0]) for i in range(len(locations_matrix))}  # (lon, lat)\n\ndef update(t):\n    ax.clear()\n    # Redraw static map features each frame\n    ax.add_feature(cfeature.COASTLINE)\n    ax.add_feature(cfeature.LAND)\n    ax.add_feature(cfeature.RIVERS)\n    ax.add_feature(cfeature.OCEAN)\n    \n    t_indices = np.where(temporal_occurance_matrix[:, t])[0]\n    t_graph = graph.subgraph(t_indices).copy()\n\n    # Plot edges manually\n    for u, v in t_graph.edges():\n        x_coords = [pos[u][0], pos[v][0]]\n        y_coords = [pos[u][1], pos[v][1]]\n        ax.plot(x_coords, y_coords, transform=ccrs.Geodetic(), color='k', linewidth=0.1)\n    \n    # Plot nodes\n    for node in t_graph.nodes():\n        lon, lat = pos[node]\n        ax.plot(lon, lat, marker='o', color='brown', transform=ccrs.Geodetic(), markersize=4)\n\n    ax.set_title(f\"Settlements Progression in Peru\\nYear: {int(timestamps[t])}\", fontsize=12)\n\n# Animate and save\nanim = FuncAnimation(fig, update, frames=10, interval=600)\nanim.save(\"temporal_graph_map.gif\", writer=PillowWriter(fps=2))\n\n# Display in notebook\nImage(filename=\"temporal_graph_map.gif\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:31:46.749861Z","iopub.execute_input":"2025-06-10T08:31:46.750161Z","iopub.status.idle":"2025-06-10T08:32:49.363831Z","shell.execute_reply.started":"2025-06-10T08:31:46.750142Z","shell.execute_reply":"2025-06-10T08:32:49.362942Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Nice! Shame we don't see the mountains info here, to conclude if these patterns of the graph are related to the elevation level (probably yes). Some things to notice here is that most of the settlements are near the water, either the river or the ocean coast, as was to be expected. Now, one idea to use these graphs is to build a regression model that predicts if at any time stamp a graph node in the graph will obtain a new neighbor or lose one of the existing ones. This could give us a clue as to where the potential lost cities could be.\n\n# Future Work\n\nThis is incomplete work, and demands additional effort. I will keep on in a separate notebook. Besides the obvious need to incorporate other regions than Peru, there are a few more important directions to explore here.\n\nFor details on this, check my next notebook [Walking Through the Hills](https://www.kaggle.com/code/rackovic1994/walking-through-the-hills)\n\n## 1. Distances\nStraight line is not a good distance measure. Especially in the ancient times. And especially in the mountain region like Peru. Distances should incorporate the info of elevation levels and rivers.\n\n## 2. Incorporating GPT\nAs mentioned earlier in this notebook, i didn't use any, because i am cheap. But it would be a good assistant in determining the foundation dates, when they are not clearly stated in the info-box or intro.\n\n## 3. Node Features\nClearly the nodes are not scattered randomly. Hills and river proximity are important. To account for this, I think the nodes should have features such as geographical altitude, and a river proximity. But for river proximity I wouldn't use straight line distance. Instead, I advocate for using a relief map, and flooding method. Seeing at which altitude of the water does the distance to the settlement significantly (and sufficiently) drop.\n\n## 4. Build a regression model\nThis is still to figure out, but the idea for now is to train a simple medel, even something like linear regression. An input would be a single node, with its features like the ones mentioned above, but also with its graph-derived features, that will change with time (degree, centrality,...). The output would be as simple as the number of neighbors to expect. Then, when a model is good enough, we can see the nodes where it 'fails', and they would be potential locations for the lost cities. ","metadata":{}}]}